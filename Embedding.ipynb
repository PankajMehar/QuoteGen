{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/all.txt','r') as datafile:\n",
    "    quotes = datafile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['04',\n",
       " '05',\n",
       " '1',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '10000seat',\n",
       " '101',\n",
       " '102',\n",
       " '105',\n",
       " '11',\n",
       " '12',\n",
       " '125',\n",
       " '13',\n",
       " '132',\n",
       " '137',\n",
       " '14',\n",
       " '1400',\n",
       " '15',\n",
       " '16',\n",
       " '1600',\n",
       " '17',\n",
       " '17th',\n",
       " '18',\n",
       " '180000',\n",
       " '1832',\n",
       " '187',\n",
       " '18karat',\n",
       " '18th',\n",
       " '19',\n",
       " '19000',\n",
       " '1911',\n",
       " '1937',\n",
       " '1948',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1965',\n",
       " '1967',\n",
       " '1969',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1980s',\n",
       " '1984',\n",
       " '1988',\n",
       " '1989',\n",
       " '19th',\n",
       " '19thcentury',\n",
       " '19year',\n",
       " '1st',\n",
       " '1yearold',\n",
       " '2',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2008',\n",
       " '200million',\n",
       " '2010',\n",
       " '2025',\n",
       " '20million',\n",
       " '20s',\n",
       " '20th',\n",
       " '20thcentury',\n",
       " '21',\n",
       " '215000',\n",
       " '21stcentury',\n",
       " '22',\n",
       " '22year',\n",
       " '24',\n",
       " '247',\n",
       " '25',\n",
       " '250000',\n",
       " '26',\n",
       " '28',\n",
       " '3',\n",
       " '30',\n",
       " '300',\n",
       " '30000',\n",
       " '30th',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '37',\n",
       " '3d',\n",
       " '3yearold',\n",
       " '4',\n",
       " '40',\n",
       " '400',\n",
       " '4000',\n",
       " '40000',\n",
       " '40hour',\n",
       " '40th',\n",
       " '42nd',\n",
       " '45',\n",
       " '48',\n",
       " '4th',\n",
       " '5',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50s',\n",
       " '50yearsold',\n",
       " '56',\n",
       " '58',\n",
       " '6',\n",
       " '60',\n",
       " '60000',\n",
       " '60s',\n",
       " '60yearold',\n",
       " '62',\n",
       " '62000',\n",
       " '67',\n",
       " '6th',\n",
       " '7',\n",
       " '70',\n",
       " '700page',\n",
       " '70s',\n",
       " '73',\n",
       " '8',\n",
       " '80',\n",
       " '800',\n",
       " '80s',\n",
       " '88',\n",
       " '9',\n",
       " '90',\n",
       " '9000',\n",
       " '90s',\n",
       " '91',\n",
       " '911',\n",
       " '92',\n",
       " '96',\n",
       " '99',\n",
       " '9k',\n",
       " 'a',\n",
       " 'aaa',\n",
       " 'ababa',\n",
       " 'aback',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandonment',\n",
       " 'abates']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(set(quotes[0].split())))[:145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = list(sorted(set(quotes[0].split())))[138:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0] = '0' \n",
    "words[1] = 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_int = dict([(w,i) for i,w in enumerate(words)])\n",
    "int_to_word = dict([(i,w) for i,w in enumerate(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0': 0,\n",
       "  'a': 1,\n",
       "  'aback': 2,\n",
       "  'abandon': 3,\n",
       "  'abandoned': 4,\n",
       "  'abandonment': 5,\n",
       "  'abates': 6,\n",
       "  'abba': 7,\n",
       "  'abbreviating': 8,\n",
       "  'abc': 9,\n",
       "  'abdicate': 10,\n",
       "  'abdicated': 11,\n",
       "  'abductions': 12,\n",
       "  'aberration': 13,\n",
       "  'abiding': 14,\n",
       "  'abilities': 15,\n",
       "  'ability': 16,\n",
       "  'abject': 17,\n",
       "  'abjure': 18,\n",
       "  'able': 19,\n",
       "  'ably': 20,\n",
       "  'abnormal': 21,\n",
       "  'abnormality': 22,\n",
       "  'abolish': 23,\n",
       "  'abolished': 24,\n",
       "  'abolition': 25,\n",
       "  'abolitionists': 26,\n",
       "  'abortion': 27,\n",
       "  'abortions': 28,\n",
       "  'about': 29,\n",
       "  'above': 30,\n",
       "  'abraham': 31,\n",
       "  'abridgement': 32,\n",
       "  'abroad': 33,\n",
       "  'abrupt': 34,\n",
       "  'absence': 35,\n",
       "  'absences': 36,\n",
       "  'absent': 37,\n",
       "  'absolute': 38,\n",
       "  'absolutely': 39,\n",
       "  'absolutes': 40,\n",
       "  'absorb': 41,\n",
       "  'absorbed': 42,\n",
       "  'abstract': 43,\n",
       "  'abstractly': 44,\n",
       "  'absurd': 45,\n",
       "  'absurdist': 46,\n",
       "  'absurdity': 47,\n",
       "  'absurdly': 48,\n",
       "  'abundance': 49,\n",
       "  'abundant': 50,\n",
       "  'abundantly': 51,\n",
       "  'abuse': 52,\n",
       "  'abuses': 53,\n",
       "  'abusive': 54,\n",
       "  'abyss': 55,\n",
       "  'academia': 56,\n",
       "  'academic': 57,\n",
       "  'academies': 58,\n",
       "  'academy': 59,\n",
       "  'accelerate': 60,\n",
       "  'accentuate': 61,\n",
       "  'accentuated': 62,\n",
       "  'accept': 63,\n",
       "  'acceptable': 64,\n",
       "  'acceptance': 65,\n",
       "  'accepted': 66,\n",
       "  'accepting': 67,\n",
       "  'accepts': 68,\n",
       "  'access': 69,\n",
       "  'accessible': 70,\n",
       "  'accident': 71,\n",
       "  'accidental': 72,\n",
       "  'accidentally': 73,\n",
       "  'accidents': 74,\n",
       "  'acclimate': 75,\n",
       "  'accolades': 76,\n",
       "  'accommodate': 77,\n",
       "  'accompanied': 78,\n",
       "  'accompanies': 79,\n",
       "  'accomplish': 80,\n",
       "  'accomplished': 81,\n",
       "  'accomplishes': 82,\n",
       "  'accomplishing': 83,\n",
       "  'accomplishment': 84,\n",
       "  'accomplishments': 85,\n",
       "  'accord': 86,\n",
       "  'according': 87,\n",
       "  'accordingly': 88,\n",
       "  'account': 89,\n",
       "  'accountability': 90,\n",
       "  'accountable': 91,\n",
       "  'accountancy': 92,\n",
       "  'accountant': 93,\n",
       "  'accounts': 94,\n",
       "  'accretion': 95,\n",
       "  'accumulates': 96,\n",
       "  'accumulation': 97,\n",
       "  'accuracy': 98,\n",
       "  'accurate': 99,\n",
       "  'accurately': 100,\n",
       "  'accuse': 101,\n",
       "  'accusing': 102,\n",
       "  'accustomed': 103,\n",
       "  'acheive': 104,\n",
       "  'aches': 105,\n",
       "  'achievable': 106,\n",
       "  'achieve': 107,\n",
       "  'achieved': 108,\n",
       "  'achievement': 109,\n",
       "  'achievements': 110,\n",
       "  'achieves': 111,\n",
       "  'achieving': 112,\n",
       "  'acids': 113,\n",
       "  'acknowledge': 114,\n",
       "  'acknowledged': 115,\n",
       "  'acknowledgement': 116,\n",
       "  'acknowledges': 117,\n",
       "  'acknowledging': 118,\n",
       "  'acquaintance': 119,\n",
       "  'acquaintances': 120,\n",
       "  'acquainted': 121,\n",
       "  'acquiesce': 122,\n",
       "  'acquire': 123,\n",
       "  'acquired': 124,\n",
       "  'acquiring': 125,\n",
       "  'acquisition': 126,\n",
       "  'acquisitiveness': 127,\n",
       "  'acquits': 128,\n",
       "  'acres': 129,\n",
       "  'acrobat': 130,\n",
       "  'across': 131,\n",
       "  'acrosstheboard': 132,\n",
       "  'act': 133,\n",
       "  'acted': 134,\n",
       "  'acting': 135,\n",
       "  'action': 136,\n",
       "  'actionable': 137,\n",
       "  'actions': 138,\n",
       "  'active': 139,\n",
       "  'actively': 140,\n",
       "  'activist': 141,\n",
       "  'activists': 142,\n",
       "  'activities': 143,\n",
       "  'activity': 144,\n",
       "  'actor': 145,\n",
       "  'actors': 146,\n",
       "  'actress': 147,\n",
       "  'actresss': 148,\n",
       "  'acts': 149,\n",
       "  'actual': 150,\n",
       "  'actuality': 151,\n",
       "  'actually': 152,\n",
       "  'acute': 153,\n",
       "  'acutely': 154,\n",
       "  'ad': 155,\n",
       "  'adam': 156,\n",
       "  'adapt': 157,\n",
       "  'adapted': 158,\n",
       "  'add': 159,\n",
       "  'added': 160,\n",
       "  'addict': 161,\n",
       "  'addicted': 162,\n",
       "  'addiction': 163,\n",
       "  'addictions': 164,\n",
       "  'addicts': 165,\n",
       "  'adding': 166,\n",
       "  'addis': 167,\n",
       "  'addition': 168,\n",
       "  'additional': 169,\n",
       "  'additions': 170,\n",
       "  'address': 171,\n",
       "  'addressed': 172,\n",
       "  'adds': 173,\n",
       "  'adept': 174,\n",
       "  'adequate': 175,\n",
       "  'adequately': 176,\n",
       "  'adherence': 177,\n",
       "  'adherents': 178,\n",
       "  'adjust': 179,\n",
       "  'adjusting': 180,\n",
       "  'adjustment': 181,\n",
       "  'administered': 182,\n",
       "  'administration': 183,\n",
       "  'administrations': 184,\n",
       "  'administratively': 185,\n",
       "  'admirable': 186,\n",
       "  'admiration': 187,\n",
       "  'admire': 188,\n",
       "  'admired': 189,\n",
       "  'admires': 190,\n",
       "  'admission': 191,\n",
       "  'admit': 192,\n",
       "  'admonish': 193,\n",
       "  'adolescence': 194,\n",
       "  'adolescent': 195,\n",
       "  'adopt': 196,\n",
       "  'adopted': 197,\n",
       "  'adopting': 198,\n",
       "  'adoption': 199,\n",
       "  'adopts': 200,\n",
       "  'adorable': 201,\n",
       "  'adoration': 202,\n",
       "  'adore': 203,\n",
       "  'adored': 204,\n",
       "  'adorers': 205,\n",
       "  'adorned': 206,\n",
       "  'adrenalin': 207,\n",
       "  'adriatic': 208,\n",
       "  'ads': 209,\n",
       "  'adult': 210,\n",
       "  'adulteration': 211,\n",
       "  'adultery': 212,\n",
       "  'adults': 213,\n",
       "  'advance': 214,\n",
       "  'advanced': 215,\n",
       "  'advancement': 216,\n",
       "  'advancements': 217,\n",
       "  'advances': 218,\n",
       "  'advancing': 219,\n",
       "  'advantage': 220,\n",
       "  'advantages': 221,\n",
       "  'adventure': 222,\n",
       "  'adventured': 223,\n",
       "  'adventures': 224,\n",
       "  'adventurous': 225,\n",
       "  'adversaries': 226,\n",
       "  'adversity': 227,\n",
       "  'advertisement': 228,\n",
       "  'advertisements': 229,\n",
       "  'advertising': 230,\n",
       "  'advice': 231,\n",
       "  'advise': 232,\n",
       "  'advised': 233,\n",
       "  'advising': 234,\n",
       "  'advisor': 235,\n",
       "  'advocate': 236,\n",
       "  'advocates': 237,\n",
       "  'advocating': 238,\n",
       "  'aegean': 239,\n",
       "  'aerodynamically': 240,\n",
       "  'aesthetic': 241,\n",
       "  'aesthetics': 242,\n",
       "  'afer': 243,\n",
       "  'affair': 244,\n",
       "  'affairs': 245,\n",
       "  'affect': 246,\n",
       "  'affectation': 247,\n",
       "  'affected': 248,\n",
       "  'affection': 249,\n",
       "  'affections': 250,\n",
       "  'affects': 251,\n",
       "  'affiliation': 252,\n",
       "  'affirmation': 253,\n",
       "  'affirmative': 254,\n",
       "  'affirmed': 255,\n",
       "  'affixed': 256,\n",
       "  'afflicted': 257,\n",
       "  'affliction': 258,\n",
       "  'afflicts': 259,\n",
       "  'afford': 260,\n",
       "  'afforded': 261,\n",
       "  'affords': 262,\n",
       "  'affront': 263,\n",
       "  'afghan': 264,\n",
       "  'afghanistan': 265,\n",
       "  'aficionados': 266,\n",
       "  'aflame': 267,\n",
       "  'afloat': 268,\n",
       "  'afraid': 269,\n",
       "  'africa': 270,\n",
       "  'african': 271,\n",
       "  'africanamerican': 272,\n",
       "  'africanamericans': 273,\n",
       "  'africans': 274,\n",
       "  'africas': 275,\n",
       "  'afro': 276,\n",
       "  'after': 277,\n",
       "  'afterglow': 278,\n",
       "  'afterlife': 279,\n",
       "  'aftermath': 280,\n",
       "  'afternoon': 281,\n",
       "  'afterthought': 282,\n",
       "  'afterward': 283,\n",
       "  'afterwards': 284,\n",
       "  'again': 285,\n",
       "  'against': 286,\n",
       "  'age': 287,\n",
       "  'aged': 288,\n",
       "  'agency': 289,\n",
       "  'agenda': 290,\n",
       "  'agent': 291,\n",
       "  'agents': 292,\n",
       "  'ages': 293,\n",
       "  'aggravated': 294,\n",
       "  'aggravating': 295,\n",
       "  'aggregate': 296,\n",
       "  'aggression': 297,\n",
       "  'aggressive': 298,\n",
       "  'aggressiveness': 299,\n",
       "  'aging': 300,\n",
       "  'agitation': 301,\n",
       "  'agnostic': 302,\n",
       "  'ago': 303,\n",
       "  'agonising': 304,\n",
       "  'agonizing': 305,\n",
       "  'agony': 306,\n",
       "  'agree': 307,\n",
       "  'agreeable': 308,\n",
       "  'agreed': 309,\n",
       "  'agreement': 310,\n",
       "  'agricultural': 311,\n",
       "  'agriculture': 312,\n",
       "  'ah': 313,\n",
       "  'ahead': 314,\n",
       "  'aid': 315,\n",
       "  'aide': 316,\n",
       "  'aids': 317,\n",
       "  'ailing': 318,\n",
       "  'aim': 319,\n",
       "  'aiming': 320,\n",
       "  'aimless': 321,\n",
       "  'aims': 322,\n",
       "  'aint': 323,\n",
       "  'air': 324,\n",
       "  'aircraft': 325,\n",
       "  'airline': 326,\n",
       "  'airplane': 327,\n",
       "  'airplanes': 328,\n",
       "  'airports': 329,\n",
       "  'airwaves': 330,\n",
       "  'aisles': 331,\n",
       "  'akin': 332,\n",
       "  'al': 333,\n",
       "  'alarm': 334,\n",
       "  'alarmed': 335,\n",
       "  'alarms': 336,\n",
       "  'alas': 337,\n",
       "  'albany': 338,\n",
       "  'album': 339,\n",
       "  'albums': 340,\n",
       "  'alcohol': 341,\n",
       "  'alcoholic': 342,\n",
       "  'alcoholics': 343,\n",
       "  'alert': 344,\n",
       "  'alex': 345,\n",
       "  'algebra': 346,\n",
       "  'algorithms': 347,\n",
       "  'ali': 348,\n",
       "  'aliceera': 349,\n",
       "  'alien': 350,\n",
       "  'alienate': 351,\n",
       "  'alienating': 352,\n",
       "  'alienation': 353,\n",
       "  'alight': 354,\n",
       "  'align': 355,\n",
       "  'alignment': 356,\n",
       "  'alike': 357,\n",
       "  'alive': 358,\n",
       "  'all': 359,\n",
       "  'alleged': 360,\n",
       "  'allegedly': 361,\n",
       "  'allegiance': 362,\n",
       "  'allen': 363,\n",
       "  'alleviating': 364,\n",
       "  'allgreat': 365,\n",
       "  'alliance': 366,\n",
       "  'alliances': 367,\n",
       "  'allied': 368,\n",
       "  'allies': 369,\n",
       "  'alljapanese': 370,\n",
       "  'allot': 371,\n",
       "  'allow': 372,\n",
       "  'allowances': 373,\n",
       "  'allowed': 374,\n",
       "  'allowing': 375,\n",
       "  'allows': 376,\n",
       "  'allpowerful': 377,\n",
       "  'alltogether': 378,\n",
       "  'allures': 379,\n",
       "  'allwhite': 380,\n",
       "  'allwise': 381,\n",
       "  'alma': 382,\n",
       "  'almighty': 383,\n",
       "  'almost': 384,\n",
       "  'almostinevitable': 385,\n",
       "  'alms': 386,\n",
       "  'alone': 387,\n",
       "  'along': 388,\n",
       "  'alongside': 389,\n",
       "  'aloof': 390,\n",
       "  'alphabetically': 391,\n",
       "  'already': 392,\n",
       "  'alright': 393,\n",
       "  'also': 394,\n",
       "  'altar': 395,\n",
       "  'alter': 396,\n",
       "  'alteration': 397,\n",
       "  'altered': 398,\n",
       "  'altering': 399,\n",
       "  'alternate': 400,\n",
       "  'alternative': 401,\n",
       "  'alternatives': 402,\n",
       "  'alters': 403,\n",
       "  'although': 404,\n",
       "  'altogether': 405,\n",
       "  'always': 406,\n",
       "  'alzheimers': 407,\n",
       "  'am': 408,\n",
       "  'amateur': 409,\n",
       "  'amazed': 410,\n",
       "  'amazement': 411,\n",
       "  'amazing': 412,\n",
       "  'amazingly': 413,\n",
       "  'amazon': 414,\n",
       "  'ambassador': 415,\n",
       "  'ambassadors': 416,\n",
       "  'ambience': 417,\n",
       "  'ambiguity': 418,\n",
       "  'ambition': 419,\n",
       "  'ambitions': 420,\n",
       "  'ambitious': 421,\n",
       "  'ambush': 422,\n",
       "  'amending': 423,\n",
       "  'amendment': 424,\n",
       "  'amendments': 425,\n",
       "  'amenities': 426,\n",
       "  'america': 427,\n",
       "  'american': 428,\n",
       "  'americans': 429,\n",
       "  'americas': 430,\n",
       "  'amiable': 431,\n",
       "  'amid': 432,\n",
       "  'amidst': 433,\n",
       "  'amin': 434,\n",
       "  'amino': 435,\n",
       "  'amis': 436,\n",
       "  'amnesia': 437,\n",
       "  'amnesty': 438,\n",
       "  'among': 439,\n",
       "  'amongst': 440,\n",
       "  'amount': 441,\n",
       "  'amounts': 442,\n",
       "  'amp': 443,\n",
       "  'amplify': 444,\n",
       "  'amply': 445,\n",
       "  'amulet': 446,\n",
       "  'amuse': 447,\n",
       "  'amused': 448,\n",
       "  'amusements': 449,\n",
       "  'amusing': 450,\n",
       "  'an': 451,\n",
       "  'analogy': 452,\n",
       "  'analyse': 453,\n",
       "  'analysis': 454,\n",
       "  'analyst': 455,\n",
       "  'analytical': 456,\n",
       "  'analytically': 457,\n",
       "  'analyze': 458,\n",
       "  'analyzing': 459,\n",
       "  'anarchist': 460,\n",
       "  'anarchists': 461,\n",
       "  'anarchy': 462,\n",
       "  'anathema': 463,\n",
       "  'anatidae': 464,\n",
       "  'ancestor': 465,\n",
       "  'ancestors': 466,\n",
       "  'ancestral': 467,\n",
       "  'ancestry': 468,\n",
       "  'anchored': 469,\n",
       "  'anchors': 470,\n",
       "  'ancient': 471,\n",
       "  'ancients': 472,\n",
       "  'and': 473,\n",
       "  'andor': 474,\n",
       "  'andrew': 475,\n",
       "  'andy': 476,\n",
       "  'anecdotal': 477,\n",
       "  'anesthesia': 478,\n",
       "  'anew': 479,\n",
       "  'angel': 480,\n",
       "  'angeles': 481,\n",
       "  'angelic': 482,\n",
       "  'angels': 483,\n",
       "  'anger': 484,\n",
       "  'angered': 485,\n",
       "  'angle': 486,\n",
       "  'anglican': 487,\n",
       "  'anglocatholic': 488,\n",
       "  'angry': 489,\n",
       "  'anguish': 490,\n",
       "  'anguished': 491,\n",
       "  'animal': 492,\n",
       "  'animals': 493,\n",
       "  'animate': 494,\n",
       "  'animated': 495,\n",
       "  'animation': 496,\n",
       "  'animosity': 497,\n",
       "  'anna': 498,\n",
       "  'anne': 499,\n",
       "  'annihilate': 500,\n",
       "  'annihilating': 501,\n",
       "  'annihilation': 502,\n",
       "  'anniversary': 503,\n",
       "  'announce': 504,\n",
       "  'announced': 505,\n",
       "  'annoy': 506,\n",
       "  'annoyance': 507,\n",
       "  'annoying': 508,\n",
       "  'annually': 509,\n",
       "  'anonymity': 510,\n",
       "  'anonymous': 511,\n",
       "  'anonymously': 512,\n",
       "  'another': 513,\n",
       "  'anothers': 514,\n",
       "  'answer': 515,\n",
       "  'answerable': 516,\n",
       "  'answered': 517,\n",
       "  'answers': 518,\n",
       "  'antagonism': 519,\n",
       "  'anterior': 520,\n",
       "  'anthem': 521,\n",
       "  'anthropologists': 522,\n",
       "  'anthropology': 523,\n",
       "  'anti': 524,\n",
       "  'anticipate': 525,\n",
       "  'anticipated': 526,\n",
       "  'anticipating': 527,\n",
       "  'anticipation': 528,\n",
       "  'anticipations': 529,\n",
       "  'antidote': 530,\n",
       "  'antigay': 531,\n",
       "  'antipathy': 532,\n",
       "  'antisemitism': 533,\n",
       "  'antithesis': 534,\n",
       "  'antithetical': 535,\n",
       "  'antoinette': 536,\n",
       "  'anvil': 537,\n",
       "  'anxiety': 538,\n",
       "  'anxious': 539,\n",
       "  'any': 540,\n",
       "  'anybody': 541,\n",
       "  'anybodys': 542,\n",
       "  'anymore': 543,\n",
       "  'anyone': 544,\n",
       "  'anyones': 545,\n",
       "  'anyplace': 546,\n",
       "  'anything': 547,\n",
       "  'anythings': 548,\n",
       "  'anytime': 549,\n",
       "  'anyway': 550,\n",
       "  'anywayive': 551,\n",
       "  'anywhere': 552,\n",
       "  'ap': 553,\n",
       "  'apart': 554,\n",
       "  'apartheid': 555,\n",
       "  'apartment': 556,\n",
       "  'apathy': 557,\n",
       "  'ape': 558,\n",
       "  'aphrodisiacal': 559,\n",
       "  'apocalypse': 560,\n",
       "  'apocalyptic': 561,\n",
       "  'apolitical': 562,\n",
       "  'apollo': 563,\n",
       "  'apologize': 564,\n",
       "  'apologizing': 565,\n",
       "  'apostle': 566,\n",
       "  'appalled': 567,\n",
       "  'appalling': 568,\n",
       "  'apparent': 569,\n",
       "  'apparently': 570,\n",
       "  'appeal': 571,\n",
       "  'appealed': 572,\n",
       "  'appealing': 573,\n",
       "  'appeals': 574,\n",
       "  'appear': 575,\n",
       "  'appearance': 576,\n",
       "  'appearances': 577,\n",
       "  'appearences': 578,\n",
       "  'appearing': 579,\n",
       "  'appears': 580,\n",
       "  'appetite': 581,\n",
       "  'appetites': 582,\n",
       "  'applaud': 583,\n",
       "  'applauded': 584,\n",
       "  'applauds': 585,\n",
       "  'applause': 586,\n",
       "  'apple': 587,\n",
       "  'apples': 588,\n",
       "  'applesauce': 589,\n",
       "  'application': 590,\n",
       "  'applications': 591,\n",
       "  'applied': 592,\n",
       "  'applies': 593,\n",
       "  'apply': 594,\n",
       "  'applying': 595,\n",
       "  'appoint': 596,\n",
       "  'appointed': 597,\n",
       "  'appointment': 598,\n",
       "  'appointments': 599,\n",
       "  'appreciate': 600,\n",
       "  'appreciated': 601,\n",
       "  'appreciating': 602,\n",
       "  'appreciation': 603,\n",
       "  'appreciative': 604,\n",
       "  'apprehend': 605,\n",
       "  'apprehension': 606,\n",
       "  'approach': 607,\n",
       "  'approached': 608,\n",
       "  'approaches': 609,\n",
       "  'approaching': 610,\n",
       "  'appropriate': 611,\n",
       "  'approval': 612,\n",
       "  'approve': 613,\n",
       "  'approved': 614,\n",
       "  'approves': 615,\n",
       "  'approximately': 616,\n",
       "  'apt': 617,\n",
       "  'aptitude': 618,\n",
       "  'aptitudes': 619,\n",
       "  'aquatic': 620,\n",
       "  'arab': 621,\n",
       "  'arabia': 622,\n",
       "  'arabic': 623,\n",
       "  'arabs': 624,\n",
       "  'arbitrary': 625,\n",
       "  'arbitration': 626,\n",
       "  'arc': 627,\n",
       "  'arcane': 628,\n",
       "  'archaeological': 629,\n",
       "  'archaeology': 630,\n",
       "  'arched': 631,\n",
       "  'archetypal': 632,\n",
       "  'archetype': 633,\n",
       "  'architect': 634,\n",
       "  'architects': 635,\n",
       "  'architecture': 636,\n",
       "  'architectures': 637,\n",
       "  'arctic': 638,\n",
       "  'ardently': 639,\n",
       "  'arduous': 640,\n",
       "  'are': 641,\n",
       "  'area': 642,\n",
       "  'areas': 643,\n",
       "  'arena': 644,\n",
       "  'arent': 645,\n",
       "  'argentina': 646,\n",
       "  'arguably': 647,\n",
       "  'argue': 648,\n",
       "  'arguing': 649,\n",
       "  'argument': 650,\n",
       "  'arguments': 651,\n",
       "  'aright': 652,\n",
       "  'arise': 653,\n",
       "  'arisen': 654,\n",
       "  'arises': 655,\n",
       "  'arising': 656,\n",
       "  'aristotle': 657,\n",
       "  'aristotles': 658,\n",
       "  'arizona': 659,\n",
       "  'arkansas': 660,\n",
       "  'arm': 661,\n",
       "  'armaments': 662,\n",
       "  'armed': 663,\n",
       "  'armenian': 664,\n",
       "  'armies': 665,\n",
       "  'armor': 666,\n",
       "  'armour': 667,\n",
       "  'armoured': 668,\n",
       "  'arms': 669,\n",
       "  'army': 670,\n",
       "  'aronofsky': 671,\n",
       "  'arose': 672,\n",
       "  'around': 673,\n",
       "  'aroused': 674,\n",
       "  'arrange': 675,\n",
       "  'arranged': 676,\n",
       "  'arrangement': 677,\n",
       "  'arranger': 678,\n",
       "  'array': 679,\n",
       "  'arresting': 680,\n",
       "  'arrival': 681,\n",
       "  'arrive': 682,\n",
       "  'arrived': 683,\n",
       "  'arrives': 684,\n",
       "  'arriving': 685,\n",
       "  'arrogance': 686,\n",
       "  'arrogant': 687,\n",
       "  'arrogantly': 688,\n",
       "  'arrow': 689,\n",
       "  'arrows': 690,\n",
       "  'art': 691,\n",
       "  'arteries': 692,\n",
       "  'artforms': 693,\n",
       "  'arthritis': 694,\n",
       "  'arthurs': 695,\n",
       "  'artichokes': 696,\n",
       "  'article': 697,\n",
       "  'articulate': 698,\n",
       "  'artifact': 699,\n",
       "  'artificial': 700,\n",
       "  'artist': 701,\n",
       "  'artistic': 702,\n",
       "  'artistry': 703,\n",
       "  'artists': 704,\n",
       "  'arts': 705,\n",
       "  'artwork': 706,\n",
       "  'as': 707,\n",
       "  'ascertained': 708,\n",
       "  'ascribe': 709,\n",
       "  'ascribed': 710,\n",
       "  'ashamed': 711,\n",
       "  'ashcroft': 712,\n",
       "  'ashes': 713,\n",
       "  'ashore': 714,\n",
       "  'ashtray': 715,\n",
       "  'asia': 716,\n",
       "  'asian': 717,\n",
       "  'aside': 718,\n",
       "  'ask': 719,\n",
       "  'asked': 720,\n",
       "  'askew': 721,\n",
       "  'asking': 722,\n",
       "  'asks': 723,\n",
       "  'asleep': 724,\n",
       "  'aspect': 725,\n",
       "  'aspects': 726,\n",
       "  'aspiration': 727,\n",
       "  'aspirational': 728,\n",
       "  'aspirations': 729,\n",
       "  'aspire': 730,\n",
       "  'aspires': 731,\n",
       "  'ass': 732,\n",
       "  'assassin': 733,\n",
       "  'assassinate': 734,\n",
       "  'assassination': 735,\n",
       "  'assassins': 736,\n",
       "  'assault': 737,\n",
       "  'assemble': 738,\n",
       "  'assembling': 739,\n",
       "  'assembly': 740,\n",
       "  'assent': 741,\n",
       "  'assert': 742,\n",
       "  'asserting': 743,\n",
       "  'assess': 744,\n",
       "  'assessment': 745,\n",
       "  'asset': 746,\n",
       "  'assign': 747,\n",
       "  'assigned': 748,\n",
       "  'assignment': 749,\n",
       "  'assignments': 750,\n",
       "  'assimilate': 751,\n",
       "  'assimilated': 752,\n",
       "  'assimilation': 753,\n",
       "  'assist': 754,\n",
       "  'assistance': 755,\n",
       "  'assistants': 756,\n",
       "  'assisted': 757,\n",
       "  'assisting': 758,\n",
       "  'associate': 759,\n",
       "  'associated': 760,\n",
       "  'associates': 761,\n",
       "  'association': 762,\n",
       "  'associations': 763,\n",
       "  'associative': 764,\n",
       "  'assume': 765,\n",
       "  'assumed': 766,\n",
       "  'assumes': 767,\n",
       "  'assumption': 768,\n",
       "  'assumptions': 769,\n",
       "  'assurances': 770,\n",
       "  'assure': 771,\n",
       "  'assured': 772,\n",
       "  'assuredly': 773,\n",
       "  'assures': 774,\n",
       "  'astaire': 775,\n",
       "  'astonished': 776,\n",
       "  'astonishing': 777,\n",
       "  'astonishment': 778,\n",
       "  'astounding': 779,\n",
       "  'astrobiology': 780,\n",
       "  'astrology': 781,\n",
       "  'astronomers': 782,\n",
       "  'astronomy': 783,\n",
       "  'astute': 784,\n",
       "  'at': 785,\n",
       "  'ate': 786,\n",
       "  'athanasian': 787,\n",
       "  'atheism': 788,\n",
       "  'atheist': 789,\n",
       "  'atheists': 790,\n",
       "  'athena': 791,\n",
       "  'athlete': 792,\n",
       "  'athletes': 793,\n",
       "  'athletic': 794,\n",
       "  'athletics': 795,\n",
       "  'atmosphere': 796,\n",
       "  'atom': 797,\n",
       "  'atomic': 798,\n",
       "  'atoms': 799,\n",
       "  'atrocities': 800,\n",
       "  'attach': 801,\n",
       "  'attached': 802,\n",
       "  'attachment': 803,\n",
       "  'attachments': 804,\n",
       "  'attack': 805,\n",
       "  'attacked': 806,\n",
       "  'attacks': 807,\n",
       "  'attain': 808,\n",
       "  'attainable': 809,\n",
       "  'attained': 810,\n",
       "  'attaining': 811,\n",
       "  'attainment': 812,\n",
       "  'attains': 813,\n",
       "  'attempt': 814,\n",
       "  'attempted': 815,\n",
       "  'attempting': 816,\n",
       "  'attempts': 817,\n",
       "  'attend': 818,\n",
       "  'attended': 819,\n",
       "  'attends': 820,\n",
       "  'attention': 821,\n",
       "  'attentive': 822,\n",
       "  'attest': 823,\n",
       "  'attic': 824,\n",
       "  'attitude': 825,\n",
       "  'attitudes': 826,\n",
       "  'attorney': 827,\n",
       "  'attorneys': 828,\n",
       "  'attract': 829,\n",
       "  'attracted': 830,\n",
       "  'attracting': 831,\n",
       "  'attraction': 832,\n",
       "  'attractions': 833,\n",
       "  'attractive': 834,\n",
       "  'attracts': 835,\n",
       "  'attributable': 836,\n",
       "  'attribute': 837,\n",
       "  'attributed': 838,\n",
       "  'attributes': 839,\n",
       "  'attribution': 840,\n",
       "  'audacious': 841,\n",
       "  'audacity': 842,\n",
       "  'audibly': 843,\n",
       "  'audience': 844,\n",
       "  'audiences': 845,\n",
       "  'audit': 846,\n",
       "  'audition': 847,\n",
       "  'auditioned': 848,\n",
       "  'audrey': 849,\n",
       "  'aught': 850,\n",
       "  'augury': 851,\n",
       "  'aunt': 852,\n",
       "  'auntie': 853,\n",
       "  'aunts': 854,\n",
       "  'aussies': 855,\n",
       "  'austerity': 856,\n",
       "  'australia': 857,\n",
       "  'australian': 858,\n",
       "  'australians': 859,\n",
       "  'australias': 860,\n",
       "  'authentic': 861,\n",
       "  'authenticity': 862,\n",
       "  'author': 863,\n",
       "  'authoritarian': 864,\n",
       "  'authorities': 865,\n",
       "  'authority': 866,\n",
       "  'authors': 867,\n",
       "  'autism': 868,\n",
       "  'autobiographies': 869,\n",
       "  'autobiography': 870,\n",
       "  'autocracy': 871,\n",
       "  'autograph': 872,\n",
       "  'automatic': 873,\n",
       "  'automatically': 874,\n",
       "  'automatons': 875,\n",
       "  'automobile': 876,\n",
       "  'autonomy': 877,\n",
       "  'autumn': 878,\n",
       "  'avail': 879,\n",
       "  'availability': 880,\n",
       "  'available': 881,\n",
       "  'avantgarde': 882,\n",
       "  'avenue': 883,\n",
       "  'average': 884,\n",
       "  'avert': 885,\n",
       "  'avocado': 886,\n",
       "  'avoid': 887,\n",
       "  'avoidance': 888,\n",
       "  'avoided': 889,\n",
       "  'avoiding': 890,\n",
       "  'avoids': 891,\n",
       "  'avowed': 892,\n",
       "  'awake': 893,\n",
       "  'awaken': 894,\n",
       "  'awakening': 895,\n",
       "  'awakens': 896,\n",
       "  'award': 897,\n",
       "  'awards': 898,\n",
       "  'aware': 899,\n",
       "  'awareness': 900,\n",
       "  'awash': 901,\n",
       "  'away': 902,\n",
       "  'awe': 903,\n",
       "  'awesome': 904,\n",
       "  'awful': 905,\n",
       "  'awfully': 906,\n",
       "  'awhile': 907,\n",
       "  'awkward': 908,\n",
       "  'awoke': 909,\n",
       "  'awry': 910,\n",
       "  'axioms': 911,\n",
       "  'ay': 912,\n",
       "  'aye': 913,\n",
       "  'ayurveda': 914,\n",
       "  'b': 915,\n",
       "  'babe': 916,\n",
       "  'babies': 917,\n",
       "  'baby': 918,\n",
       "  'babysitters': 919,\n",
       "  'bachelor': 920,\n",
       "  'bachelors': 921,\n",
       "  'back': 922,\n",
       "  'backbone': 923,\n",
       "  'backburner': 924,\n",
       "  'backed': 925,\n",
       "  'background': 926,\n",
       "  'backgrounds': 927,\n",
       "  'backing': 928,\n",
       "  'backpack': 929,\n",
       "  'backrooms': 930,\n",
       "  'backs': 931,\n",
       "  'backside': 932,\n",
       "  'backstage': 933,\n",
       "  'backward': 934,\n",
       "  'backwards': 935,\n",
       "  'bacon': 936,\n",
       "  'bad': 937,\n",
       "  'baddest': 938,\n",
       "  'badly': 939,\n",
       "  'badmouthing': 940,\n",
       "  'baffled': 941,\n",
       "  'baffling': 942,\n",
       "  'bag': 943,\n",
       "  'bagels': 944,\n",
       "  'baggy': 945,\n",
       "  'bags': 946,\n",
       "  'bailed': 947,\n",
       "  'bailiff': 948,\n",
       "  'bait': 949,\n",
       "  'baitandswitch': 950,\n",
       "  'bakers': 951,\n",
       "  'bakery': 952,\n",
       "  'balance': 953,\n",
       "  'balanced': 954,\n",
       "  'balances': 955,\n",
       "  'balancing': 956,\n",
       "  'balcony': 957,\n",
       "  'bald': 958,\n",
       "  'ball': 959,\n",
       "  'ballet': 960,\n",
       "  'ballistics': 961,\n",
       "  'balloon': 962,\n",
       "  'balloons': 963,\n",
       "  'ballot': 964,\n",
       "  'ballpark': 965,\n",
       "  'ballplayer': 966,\n",
       "  'balls': 967,\n",
       "  'ballsy': 968,\n",
       "  'ballymena': 969,\n",
       "  'balm': 970,\n",
       "  'baltic': 971,\n",
       "  'banal': 972,\n",
       "  'banana': 973,\n",
       "  'bananas': 974,\n",
       "  'band': 975,\n",
       "  'bandage': 976,\n",
       "  'bands': 977,\n",
       "  'bang': 978,\n",
       "  'bangalore': 979,\n",
       "  'bangs': 980,\n",
       "  'banish': 981,\n",
       "  'banished': 982,\n",
       "  'banishment': 983,\n",
       "  'bank': 984,\n",
       "  'bankrupt': 985,\n",
       "  'bankruptcy': 986,\n",
       "  'banks': 987,\n",
       "  'banners': 988,\n",
       "  'banoodles': 989,\n",
       "  'banquet': 990,\n",
       "  'banter': 991,\n",
       "  'bar': 992,\n",
       "  'barack': 993,\n",
       "  'baracks': 994,\n",
       "  'barbara': 995,\n",
       "  'barbarism': 996,\n",
       "  'barbarous': 997,\n",
       "  'barbie': 998,\n",
       "  'barbra': 999,\n",
       "  ...},\n",
       " {0: '0',\n",
       "  1: 'a',\n",
       "  2: 'aback',\n",
       "  3: 'abandon',\n",
       "  4: 'abandoned',\n",
       "  5: 'abandonment',\n",
       "  6: 'abates',\n",
       "  7: 'abba',\n",
       "  8: 'abbreviating',\n",
       "  9: 'abc',\n",
       "  10: 'abdicate',\n",
       "  11: 'abdicated',\n",
       "  12: 'abductions',\n",
       "  13: 'aberration',\n",
       "  14: 'abiding',\n",
       "  15: 'abilities',\n",
       "  16: 'ability',\n",
       "  17: 'abject',\n",
       "  18: 'abjure',\n",
       "  19: 'able',\n",
       "  20: 'ably',\n",
       "  21: 'abnormal',\n",
       "  22: 'abnormality',\n",
       "  23: 'abolish',\n",
       "  24: 'abolished',\n",
       "  25: 'abolition',\n",
       "  26: 'abolitionists',\n",
       "  27: 'abortion',\n",
       "  28: 'abortions',\n",
       "  29: 'about',\n",
       "  30: 'above',\n",
       "  31: 'abraham',\n",
       "  32: 'abridgement',\n",
       "  33: 'abroad',\n",
       "  34: 'abrupt',\n",
       "  35: 'absence',\n",
       "  36: 'absences',\n",
       "  37: 'absent',\n",
       "  38: 'absolute',\n",
       "  39: 'absolutely',\n",
       "  40: 'absolutes',\n",
       "  41: 'absorb',\n",
       "  42: 'absorbed',\n",
       "  43: 'abstract',\n",
       "  44: 'abstractly',\n",
       "  45: 'absurd',\n",
       "  46: 'absurdist',\n",
       "  47: 'absurdity',\n",
       "  48: 'absurdly',\n",
       "  49: 'abundance',\n",
       "  50: 'abundant',\n",
       "  51: 'abundantly',\n",
       "  52: 'abuse',\n",
       "  53: 'abuses',\n",
       "  54: 'abusive',\n",
       "  55: 'abyss',\n",
       "  56: 'academia',\n",
       "  57: 'academic',\n",
       "  58: 'academies',\n",
       "  59: 'academy',\n",
       "  60: 'accelerate',\n",
       "  61: 'accentuate',\n",
       "  62: 'accentuated',\n",
       "  63: 'accept',\n",
       "  64: 'acceptable',\n",
       "  65: 'acceptance',\n",
       "  66: 'accepted',\n",
       "  67: 'accepting',\n",
       "  68: 'accepts',\n",
       "  69: 'access',\n",
       "  70: 'accessible',\n",
       "  71: 'accident',\n",
       "  72: 'accidental',\n",
       "  73: 'accidentally',\n",
       "  74: 'accidents',\n",
       "  75: 'acclimate',\n",
       "  76: 'accolades',\n",
       "  77: 'accommodate',\n",
       "  78: 'accompanied',\n",
       "  79: 'accompanies',\n",
       "  80: 'accomplish',\n",
       "  81: 'accomplished',\n",
       "  82: 'accomplishes',\n",
       "  83: 'accomplishing',\n",
       "  84: 'accomplishment',\n",
       "  85: 'accomplishments',\n",
       "  86: 'accord',\n",
       "  87: 'according',\n",
       "  88: 'accordingly',\n",
       "  89: 'account',\n",
       "  90: 'accountability',\n",
       "  91: 'accountable',\n",
       "  92: 'accountancy',\n",
       "  93: 'accountant',\n",
       "  94: 'accounts',\n",
       "  95: 'accretion',\n",
       "  96: 'accumulates',\n",
       "  97: 'accumulation',\n",
       "  98: 'accuracy',\n",
       "  99: 'accurate',\n",
       "  100: 'accurately',\n",
       "  101: 'accuse',\n",
       "  102: 'accusing',\n",
       "  103: 'accustomed',\n",
       "  104: 'acheive',\n",
       "  105: 'aches',\n",
       "  106: 'achievable',\n",
       "  107: 'achieve',\n",
       "  108: 'achieved',\n",
       "  109: 'achievement',\n",
       "  110: 'achievements',\n",
       "  111: 'achieves',\n",
       "  112: 'achieving',\n",
       "  113: 'acids',\n",
       "  114: 'acknowledge',\n",
       "  115: 'acknowledged',\n",
       "  116: 'acknowledgement',\n",
       "  117: 'acknowledges',\n",
       "  118: 'acknowledging',\n",
       "  119: 'acquaintance',\n",
       "  120: 'acquaintances',\n",
       "  121: 'acquainted',\n",
       "  122: 'acquiesce',\n",
       "  123: 'acquire',\n",
       "  124: 'acquired',\n",
       "  125: 'acquiring',\n",
       "  126: 'acquisition',\n",
       "  127: 'acquisitiveness',\n",
       "  128: 'acquits',\n",
       "  129: 'acres',\n",
       "  130: 'acrobat',\n",
       "  131: 'across',\n",
       "  132: 'acrosstheboard',\n",
       "  133: 'act',\n",
       "  134: 'acted',\n",
       "  135: 'acting',\n",
       "  136: 'action',\n",
       "  137: 'actionable',\n",
       "  138: 'actions',\n",
       "  139: 'active',\n",
       "  140: 'actively',\n",
       "  141: 'activist',\n",
       "  142: 'activists',\n",
       "  143: 'activities',\n",
       "  144: 'activity',\n",
       "  145: 'actor',\n",
       "  146: 'actors',\n",
       "  147: 'actress',\n",
       "  148: 'actresss',\n",
       "  149: 'acts',\n",
       "  150: 'actual',\n",
       "  151: 'actuality',\n",
       "  152: 'actually',\n",
       "  153: 'acute',\n",
       "  154: 'acutely',\n",
       "  155: 'ad',\n",
       "  156: 'adam',\n",
       "  157: 'adapt',\n",
       "  158: 'adapted',\n",
       "  159: 'add',\n",
       "  160: 'added',\n",
       "  161: 'addict',\n",
       "  162: 'addicted',\n",
       "  163: 'addiction',\n",
       "  164: 'addictions',\n",
       "  165: 'addicts',\n",
       "  166: 'adding',\n",
       "  167: 'addis',\n",
       "  168: 'addition',\n",
       "  169: 'additional',\n",
       "  170: 'additions',\n",
       "  171: 'address',\n",
       "  172: 'addressed',\n",
       "  173: 'adds',\n",
       "  174: 'adept',\n",
       "  175: 'adequate',\n",
       "  176: 'adequately',\n",
       "  177: 'adherence',\n",
       "  178: 'adherents',\n",
       "  179: 'adjust',\n",
       "  180: 'adjusting',\n",
       "  181: 'adjustment',\n",
       "  182: 'administered',\n",
       "  183: 'administration',\n",
       "  184: 'administrations',\n",
       "  185: 'administratively',\n",
       "  186: 'admirable',\n",
       "  187: 'admiration',\n",
       "  188: 'admire',\n",
       "  189: 'admired',\n",
       "  190: 'admires',\n",
       "  191: 'admission',\n",
       "  192: 'admit',\n",
       "  193: 'admonish',\n",
       "  194: 'adolescence',\n",
       "  195: 'adolescent',\n",
       "  196: 'adopt',\n",
       "  197: 'adopted',\n",
       "  198: 'adopting',\n",
       "  199: 'adoption',\n",
       "  200: 'adopts',\n",
       "  201: 'adorable',\n",
       "  202: 'adoration',\n",
       "  203: 'adore',\n",
       "  204: 'adored',\n",
       "  205: 'adorers',\n",
       "  206: 'adorned',\n",
       "  207: 'adrenalin',\n",
       "  208: 'adriatic',\n",
       "  209: 'ads',\n",
       "  210: 'adult',\n",
       "  211: 'adulteration',\n",
       "  212: 'adultery',\n",
       "  213: 'adults',\n",
       "  214: 'advance',\n",
       "  215: 'advanced',\n",
       "  216: 'advancement',\n",
       "  217: 'advancements',\n",
       "  218: 'advances',\n",
       "  219: 'advancing',\n",
       "  220: 'advantage',\n",
       "  221: 'advantages',\n",
       "  222: 'adventure',\n",
       "  223: 'adventured',\n",
       "  224: 'adventures',\n",
       "  225: 'adventurous',\n",
       "  226: 'adversaries',\n",
       "  227: 'adversity',\n",
       "  228: 'advertisement',\n",
       "  229: 'advertisements',\n",
       "  230: 'advertising',\n",
       "  231: 'advice',\n",
       "  232: 'advise',\n",
       "  233: 'advised',\n",
       "  234: 'advising',\n",
       "  235: 'advisor',\n",
       "  236: 'advocate',\n",
       "  237: 'advocates',\n",
       "  238: 'advocating',\n",
       "  239: 'aegean',\n",
       "  240: 'aerodynamically',\n",
       "  241: 'aesthetic',\n",
       "  242: 'aesthetics',\n",
       "  243: 'afer',\n",
       "  244: 'affair',\n",
       "  245: 'affairs',\n",
       "  246: 'affect',\n",
       "  247: 'affectation',\n",
       "  248: 'affected',\n",
       "  249: 'affection',\n",
       "  250: 'affections',\n",
       "  251: 'affects',\n",
       "  252: 'affiliation',\n",
       "  253: 'affirmation',\n",
       "  254: 'affirmative',\n",
       "  255: 'affirmed',\n",
       "  256: 'affixed',\n",
       "  257: 'afflicted',\n",
       "  258: 'affliction',\n",
       "  259: 'afflicts',\n",
       "  260: 'afford',\n",
       "  261: 'afforded',\n",
       "  262: 'affords',\n",
       "  263: 'affront',\n",
       "  264: 'afghan',\n",
       "  265: 'afghanistan',\n",
       "  266: 'aficionados',\n",
       "  267: 'aflame',\n",
       "  268: 'afloat',\n",
       "  269: 'afraid',\n",
       "  270: 'africa',\n",
       "  271: 'african',\n",
       "  272: 'africanamerican',\n",
       "  273: 'africanamericans',\n",
       "  274: 'africans',\n",
       "  275: 'africas',\n",
       "  276: 'afro',\n",
       "  277: 'after',\n",
       "  278: 'afterglow',\n",
       "  279: 'afterlife',\n",
       "  280: 'aftermath',\n",
       "  281: 'afternoon',\n",
       "  282: 'afterthought',\n",
       "  283: 'afterward',\n",
       "  284: 'afterwards',\n",
       "  285: 'again',\n",
       "  286: 'against',\n",
       "  287: 'age',\n",
       "  288: 'aged',\n",
       "  289: 'agency',\n",
       "  290: 'agenda',\n",
       "  291: 'agent',\n",
       "  292: 'agents',\n",
       "  293: 'ages',\n",
       "  294: 'aggravated',\n",
       "  295: 'aggravating',\n",
       "  296: 'aggregate',\n",
       "  297: 'aggression',\n",
       "  298: 'aggressive',\n",
       "  299: 'aggressiveness',\n",
       "  300: 'aging',\n",
       "  301: 'agitation',\n",
       "  302: 'agnostic',\n",
       "  303: 'ago',\n",
       "  304: 'agonising',\n",
       "  305: 'agonizing',\n",
       "  306: 'agony',\n",
       "  307: 'agree',\n",
       "  308: 'agreeable',\n",
       "  309: 'agreed',\n",
       "  310: 'agreement',\n",
       "  311: 'agricultural',\n",
       "  312: 'agriculture',\n",
       "  313: 'ah',\n",
       "  314: 'ahead',\n",
       "  315: 'aid',\n",
       "  316: 'aide',\n",
       "  317: 'aids',\n",
       "  318: 'ailing',\n",
       "  319: 'aim',\n",
       "  320: 'aiming',\n",
       "  321: 'aimless',\n",
       "  322: 'aims',\n",
       "  323: 'aint',\n",
       "  324: 'air',\n",
       "  325: 'aircraft',\n",
       "  326: 'airline',\n",
       "  327: 'airplane',\n",
       "  328: 'airplanes',\n",
       "  329: 'airports',\n",
       "  330: 'airwaves',\n",
       "  331: 'aisles',\n",
       "  332: 'akin',\n",
       "  333: 'al',\n",
       "  334: 'alarm',\n",
       "  335: 'alarmed',\n",
       "  336: 'alarms',\n",
       "  337: 'alas',\n",
       "  338: 'albany',\n",
       "  339: 'album',\n",
       "  340: 'albums',\n",
       "  341: 'alcohol',\n",
       "  342: 'alcoholic',\n",
       "  343: 'alcoholics',\n",
       "  344: 'alert',\n",
       "  345: 'alex',\n",
       "  346: 'algebra',\n",
       "  347: 'algorithms',\n",
       "  348: 'ali',\n",
       "  349: 'aliceera',\n",
       "  350: 'alien',\n",
       "  351: 'alienate',\n",
       "  352: 'alienating',\n",
       "  353: 'alienation',\n",
       "  354: 'alight',\n",
       "  355: 'align',\n",
       "  356: 'alignment',\n",
       "  357: 'alike',\n",
       "  358: 'alive',\n",
       "  359: 'all',\n",
       "  360: 'alleged',\n",
       "  361: 'allegedly',\n",
       "  362: 'allegiance',\n",
       "  363: 'allen',\n",
       "  364: 'alleviating',\n",
       "  365: 'allgreat',\n",
       "  366: 'alliance',\n",
       "  367: 'alliances',\n",
       "  368: 'allied',\n",
       "  369: 'allies',\n",
       "  370: 'alljapanese',\n",
       "  371: 'allot',\n",
       "  372: 'allow',\n",
       "  373: 'allowances',\n",
       "  374: 'allowed',\n",
       "  375: 'allowing',\n",
       "  376: 'allows',\n",
       "  377: 'allpowerful',\n",
       "  378: 'alltogether',\n",
       "  379: 'allures',\n",
       "  380: 'allwhite',\n",
       "  381: 'allwise',\n",
       "  382: 'alma',\n",
       "  383: 'almighty',\n",
       "  384: 'almost',\n",
       "  385: 'almostinevitable',\n",
       "  386: 'alms',\n",
       "  387: 'alone',\n",
       "  388: 'along',\n",
       "  389: 'alongside',\n",
       "  390: 'aloof',\n",
       "  391: 'alphabetically',\n",
       "  392: 'already',\n",
       "  393: 'alright',\n",
       "  394: 'also',\n",
       "  395: 'altar',\n",
       "  396: 'alter',\n",
       "  397: 'alteration',\n",
       "  398: 'altered',\n",
       "  399: 'altering',\n",
       "  400: 'alternate',\n",
       "  401: 'alternative',\n",
       "  402: 'alternatives',\n",
       "  403: 'alters',\n",
       "  404: 'although',\n",
       "  405: 'altogether',\n",
       "  406: 'always',\n",
       "  407: 'alzheimers',\n",
       "  408: 'am',\n",
       "  409: 'amateur',\n",
       "  410: 'amazed',\n",
       "  411: 'amazement',\n",
       "  412: 'amazing',\n",
       "  413: 'amazingly',\n",
       "  414: 'amazon',\n",
       "  415: 'ambassador',\n",
       "  416: 'ambassadors',\n",
       "  417: 'ambience',\n",
       "  418: 'ambiguity',\n",
       "  419: 'ambition',\n",
       "  420: 'ambitions',\n",
       "  421: 'ambitious',\n",
       "  422: 'ambush',\n",
       "  423: 'amending',\n",
       "  424: 'amendment',\n",
       "  425: 'amendments',\n",
       "  426: 'amenities',\n",
       "  427: 'america',\n",
       "  428: 'american',\n",
       "  429: 'americans',\n",
       "  430: 'americas',\n",
       "  431: 'amiable',\n",
       "  432: 'amid',\n",
       "  433: 'amidst',\n",
       "  434: 'amin',\n",
       "  435: 'amino',\n",
       "  436: 'amis',\n",
       "  437: 'amnesia',\n",
       "  438: 'amnesty',\n",
       "  439: 'among',\n",
       "  440: 'amongst',\n",
       "  441: 'amount',\n",
       "  442: 'amounts',\n",
       "  443: 'amp',\n",
       "  444: 'amplify',\n",
       "  445: 'amply',\n",
       "  446: 'amulet',\n",
       "  447: 'amuse',\n",
       "  448: 'amused',\n",
       "  449: 'amusements',\n",
       "  450: 'amusing',\n",
       "  451: 'an',\n",
       "  452: 'analogy',\n",
       "  453: 'analyse',\n",
       "  454: 'analysis',\n",
       "  455: 'analyst',\n",
       "  456: 'analytical',\n",
       "  457: 'analytically',\n",
       "  458: 'analyze',\n",
       "  459: 'analyzing',\n",
       "  460: 'anarchist',\n",
       "  461: 'anarchists',\n",
       "  462: 'anarchy',\n",
       "  463: 'anathema',\n",
       "  464: 'anatidae',\n",
       "  465: 'ancestor',\n",
       "  466: 'ancestors',\n",
       "  467: 'ancestral',\n",
       "  468: 'ancestry',\n",
       "  469: 'anchored',\n",
       "  470: 'anchors',\n",
       "  471: 'ancient',\n",
       "  472: 'ancients',\n",
       "  473: 'and',\n",
       "  474: 'andor',\n",
       "  475: 'andrew',\n",
       "  476: 'andy',\n",
       "  477: 'anecdotal',\n",
       "  478: 'anesthesia',\n",
       "  479: 'anew',\n",
       "  480: 'angel',\n",
       "  481: 'angeles',\n",
       "  482: 'angelic',\n",
       "  483: 'angels',\n",
       "  484: 'anger',\n",
       "  485: 'angered',\n",
       "  486: 'angle',\n",
       "  487: 'anglican',\n",
       "  488: 'anglocatholic',\n",
       "  489: 'angry',\n",
       "  490: 'anguish',\n",
       "  491: 'anguished',\n",
       "  492: 'animal',\n",
       "  493: 'animals',\n",
       "  494: 'animate',\n",
       "  495: 'animated',\n",
       "  496: 'animation',\n",
       "  497: 'animosity',\n",
       "  498: 'anna',\n",
       "  499: 'anne',\n",
       "  500: 'annihilate',\n",
       "  501: 'annihilating',\n",
       "  502: 'annihilation',\n",
       "  503: 'anniversary',\n",
       "  504: 'announce',\n",
       "  505: 'announced',\n",
       "  506: 'annoy',\n",
       "  507: 'annoyance',\n",
       "  508: 'annoying',\n",
       "  509: 'annually',\n",
       "  510: 'anonymity',\n",
       "  511: 'anonymous',\n",
       "  512: 'anonymously',\n",
       "  513: 'another',\n",
       "  514: 'anothers',\n",
       "  515: 'answer',\n",
       "  516: 'answerable',\n",
       "  517: 'answered',\n",
       "  518: 'answers',\n",
       "  519: 'antagonism',\n",
       "  520: 'anterior',\n",
       "  521: 'anthem',\n",
       "  522: 'anthropologists',\n",
       "  523: 'anthropology',\n",
       "  524: 'anti',\n",
       "  525: 'anticipate',\n",
       "  526: 'anticipated',\n",
       "  527: 'anticipating',\n",
       "  528: 'anticipation',\n",
       "  529: 'anticipations',\n",
       "  530: 'antidote',\n",
       "  531: 'antigay',\n",
       "  532: 'antipathy',\n",
       "  533: 'antisemitism',\n",
       "  534: 'antithesis',\n",
       "  535: 'antithetical',\n",
       "  536: 'antoinette',\n",
       "  537: 'anvil',\n",
       "  538: 'anxiety',\n",
       "  539: 'anxious',\n",
       "  540: 'any',\n",
       "  541: 'anybody',\n",
       "  542: 'anybodys',\n",
       "  543: 'anymore',\n",
       "  544: 'anyone',\n",
       "  545: 'anyones',\n",
       "  546: 'anyplace',\n",
       "  547: 'anything',\n",
       "  548: 'anythings',\n",
       "  549: 'anytime',\n",
       "  550: 'anyway',\n",
       "  551: 'anywayive',\n",
       "  552: 'anywhere',\n",
       "  553: 'ap',\n",
       "  554: 'apart',\n",
       "  555: 'apartheid',\n",
       "  556: 'apartment',\n",
       "  557: 'apathy',\n",
       "  558: 'ape',\n",
       "  559: 'aphrodisiacal',\n",
       "  560: 'apocalypse',\n",
       "  561: 'apocalyptic',\n",
       "  562: 'apolitical',\n",
       "  563: 'apollo',\n",
       "  564: 'apologize',\n",
       "  565: 'apologizing',\n",
       "  566: 'apostle',\n",
       "  567: 'appalled',\n",
       "  568: 'appalling',\n",
       "  569: 'apparent',\n",
       "  570: 'apparently',\n",
       "  571: 'appeal',\n",
       "  572: 'appealed',\n",
       "  573: 'appealing',\n",
       "  574: 'appeals',\n",
       "  575: 'appear',\n",
       "  576: 'appearance',\n",
       "  577: 'appearances',\n",
       "  578: 'appearences',\n",
       "  579: 'appearing',\n",
       "  580: 'appears',\n",
       "  581: 'appetite',\n",
       "  582: 'appetites',\n",
       "  583: 'applaud',\n",
       "  584: 'applauded',\n",
       "  585: 'applauds',\n",
       "  586: 'applause',\n",
       "  587: 'apple',\n",
       "  588: 'apples',\n",
       "  589: 'applesauce',\n",
       "  590: 'application',\n",
       "  591: 'applications',\n",
       "  592: 'applied',\n",
       "  593: 'applies',\n",
       "  594: 'apply',\n",
       "  595: 'applying',\n",
       "  596: 'appoint',\n",
       "  597: 'appointed',\n",
       "  598: 'appointment',\n",
       "  599: 'appointments',\n",
       "  600: 'appreciate',\n",
       "  601: 'appreciated',\n",
       "  602: 'appreciating',\n",
       "  603: 'appreciation',\n",
       "  604: 'appreciative',\n",
       "  605: 'apprehend',\n",
       "  606: 'apprehension',\n",
       "  607: 'approach',\n",
       "  608: 'approached',\n",
       "  609: 'approaches',\n",
       "  610: 'approaching',\n",
       "  611: 'appropriate',\n",
       "  612: 'approval',\n",
       "  613: 'approve',\n",
       "  614: 'approved',\n",
       "  615: 'approves',\n",
       "  616: 'approximately',\n",
       "  617: 'apt',\n",
       "  618: 'aptitude',\n",
       "  619: 'aptitudes',\n",
       "  620: 'aquatic',\n",
       "  621: 'arab',\n",
       "  622: 'arabia',\n",
       "  623: 'arabic',\n",
       "  624: 'arabs',\n",
       "  625: 'arbitrary',\n",
       "  626: 'arbitration',\n",
       "  627: 'arc',\n",
       "  628: 'arcane',\n",
       "  629: 'archaeological',\n",
       "  630: 'archaeology',\n",
       "  631: 'arched',\n",
       "  632: 'archetypal',\n",
       "  633: 'archetype',\n",
       "  634: 'architect',\n",
       "  635: 'architects',\n",
       "  636: 'architecture',\n",
       "  637: 'architectures',\n",
       "  638: 'arctic',\n",
       "  639: 'ardently',\n",
       "  640: 'arduous',\n",
       "  641: 'are',\n",
       "  642: 'area',\n",
       "  643: 'areas',\n",
       "  644: 'arena',\n",
       "  645: 'arent',\n",
       "  646: 'argentina',\n",
       "  647: 'arguably',\n",
       "  648: 'argue',\n",
       "  649: 'arguing',\n",
       "  650: 'argument',\n",
       "  651: 'arguments',\n",
       "  652: 'aright',\n",
       "  653: 'arise',\n",
       "  654: 'arisen',\n",
       "  655: 'arises',\n",
       "  656: 'arising',\n",
       "  657: 'aristotle',\n",
       "  658: 'aristotles',\n",
       "  659: 'arizona',\n",
       "  660: 'arkansas',\n",
       "  661: 'arm',\n",
       "  662: 'armaments',\n",
       "  663: 'armed',\n",
       "  664: 'armenian',\n",
       "  665: 'armies',\n",
       "  666: 'armor',\n",
       "  667: 'armour',\n",
       "  668: 'armoured',\n",
       "  669: 'arms',\n",
       "  670: 'army',\n",
       "  671: 'aronofsky',\n",
       "  672: 'arose',\n",
       "  673: 'around',\n",
       "  674: 'aroused',\n",
       "  675: 'arrange',\n",
       "  676: 'arranged',\n",
       "  677: 'arrangement',\n",
       "  678: 'arranger',\n",
       "  679: 'array',\n",
       "  680: 'arresting',\n",
       "  681: 'arrival',\n",
       "  682: 'arrive',\n",
       "  683: 'arrived',\n",
       "  684: 'arrives',\n",
       "  685: 'arriving',\n",
       "  686: 'arrogance',\n",
       "  687: 'arrogant',\n",
       "  688: 'arrogantly',\n",
       "  689: 'arrow',\n",
       "  690: 'arrows',\n",
       "  691: 'art',\n",
       "  692: 'arteries',\n",
       "  693: 'artforms',\n",
       "  694: 'arthritis',\n",
       "  695: 'arthurs',\n",
       "  696: 'artichokes',\n",
       "  697: 'article',\n",
       "  698: 'articulate',\n",
       "  699: 'artifact',\n",
       "  700: 'artificial',\n",
       "  701: 'artist',\n",
       "  702: 'artistic',\n",
       "  703: 'artistry',\n",
       "  704: 'artists',\n",
       "  705: 'arts',\n",
       "  706: 'artwork',\n",
       "  707: 'as',\n",
       "  708: 'ascertained',\n",
       "  709: 'ascribe',\n",
       "  710: 'ascribed',\n",
       "  711: 'ashamed',\n",
       "  712: 'ashcroft',\n",
       "  713: 'ashes',\n",
       "  714: 'ashore',\n",
       "  715: 'ashtray',\n",
       "  716: 'asia',\n",
       "  717: 'asian',\n",
       "  718: 'aside',\n",
       "  719: 'ask',\n",
       "  720: 'asked',\n",
       "  721: 'askew',\n",
       "  722: 'asking',\n",
       "  723: 'asks',\n",
       "  724: 'asleep',\n",
       "  725: 'aspect',\n",
       "  726: 'aspects',\n",
       "  727: 'aspiration',\n",
       "  728: 'aspirational',\n",
       "  729: 'aspirations',\n",
       "  730: 'aspire',\n",
       "  731: 'aspires',\n",
       "  732: 'ass',\n",
       "  733: 'assassin',\n",
       "  734: 'assassinate',\n",
       "  735: 'assassination',\n",
       "  736: 'assassins',\n",
       "  737: 'assault',\n",
       "  738: 'assemble',\n",
       "  739: 'assembling',\n",
       "  740: 'assembly',\n",
       "  741: 'assent',\n",
       "  742: 'assert',\n",
       "  743: 'asserting',\n",
       "  744: 'assess',\n",
       "  745: 'assessment',\n",
       "  746: 'asset',\n",
       "  747: 'assign',\n",
       "  748: 'assigned',\n",
       "  749: 'assignment',\n",
       "  750: 'assignments',\n",
       "  751: 'assimilate',\n",
       "  752: 'assimilated',\n",
       "  753: 'assimilation',\n",
       "  754: 'assist',\n",
       "  755: 'assistance',\n",
       "  756: 'assistants',\n",
       "  757: 'assisted',\n",
       "  758: 'assisting',\n",
       "  759: 'associate',\n",
       "  760: 'associated',\n",
       "  761: 'associates',\n",
       "  762: 'association',\n",
       "  763: 'associations',\n",
       "  764: 'associative',\n",
       "  765: 'assume',\n",
       "  766: 'assumed',\n",
       "  767: 'assumes',\n",
       "  768: 'assumption',\n",
       "  769: 'assumptions',\n",
       "  770: 'assurances',\n",
       "  771: 'assure',\n",
       "  772: 'assured',\n",
       "  773: 'assuredly',\n",
       "  774: 'assures',\n",
       "  775: 'astaire',\n",
       "  776: 'astonished',\n",
       "  777: 'astonishing',\n",
       "  778: 'astonishment',\n",
       "  779: 'astounding',\n",
       "  780: 'astrobiology',\n",
       "  781: 'astrology',\n",
       "  782: 'astronomers',\n",
       "  783: 'astronomy',\n",
       "  784: 'astute',\n",
       "  785: 'at',\n",
       "  786: 'ate',\n",
       "  787: 'athanasian',\n",
       "  788: 'atheism',\n",
       "  789: 'atheist',\n",
       "  790: 'atheists',\n",
       "  791: 'athena',\n",
       "  792: 'athlete',\n",
       "  793: 'athletes',\n",
       "  794: 'athletic',\n",
       "  795: 'athletics',\n",
       "  796: 'atmosphere',\n",
       "  797: 'atom',\n",
       "  798: 'atomic',\n",
       "  799: 'atoms',\n",
       "  800: 'atrocities',\n",
       "  801: 'attach',\n",
       "  802: 'attached',\n",
       "  803: 'attachment',\n",
       "  804: 'attachments',\n",
       "  805: 'attack',\n",
       "  806: 'attacked',\n",
       "  807: 'attacks',\n",
       "  808: 'attain',\n",
       "  809: 'attainable',\n",
       "  810: 'attained',\n",
       "  811: 'attaining',\n",
       "  812: 'attainment',\n",
       "  813: 'attains',\n",
       "  814: 'attempt',\n",
       "  815: 'attempted',\n",
       "  816: 'attempting',\n",
       "  817: 'attempts',\n",
       "  818: 'attend',\n",
       "  819: 'attended',\n",
       "  820: 'attends',\n",
       "  821: 'attention',\n",
       "  822: 'attentive',\n",
       "  823: 'attest',\n",
       "  824: 'attic',\n",
       "  825: 'attitude',\n",
       "  826: 'attitudes',\n",
       "  827: 'attorney',\n",
       "  828: 'attorneys',\n",
       "  829: 'attract',\n",
       "  830: 'attracted',\n",
       "  831: 'attracting',\n",
       "  832: 'attraction',\n",
       "  833: 'attractions',\n",
       "  834: 'attractive',\n",
       "  835: 'attracts',\n",
       "  836: 'attributable',\n",
       "  837: 'attribute',\n",
       "  838: 'attributed',\n",
       "  839: 'attributes',\n",
       "  840: 'attribution',\n",
       "  841: 'audacious',\n",
       "  842: 'audacity',\n",
       "  843: 'audibly',\n",
       "  844: 'audience',\n",
       "  845: 'audiences',\n",
       "  846: 'audit',\n",
       "  847: 'audition',\n",
       "  848: 'auditioned',\n",
       "  849: 'audrey',\n",
       "  850: 'aught',\n",
       "  851: 'augury',\n",
       "  852: 'aunt',\n",
       "  853: 'auntie',\n",
       "  854: 'aunts',\n",
       "  855: 'aussies',\n",
       "  856: 'austerity',\n",
       "  857: 'australia',\n",
       "  858: 'australian',\n",
       "  859: 'australians',\n",
       "  860: 'australias',\n",
       "  861: 'authentic',\n",
       "  862: 'authenticity',\n",
       "  863: 'author',\n",
       "  864: 'authoritarian',\n",
       "  865: 'authorities',\n",
       "  866: 'authority',\n",
       "  867: 'authors',\n",
       "  868: 'autism',\n",
       "  869: 'autobiographies',\n",
       "  870: 'autobiography',\n",
       "  871: 'autocracy',\n",
       "  872: 'autograph',\n",
       "  873: 'automatic',\n",
       "  874: 'automatically',\n",
       "  875: 'automatons',\n",
       "  876: 'automobile',\n",
       "  877: 'autonomy',\n",
       "  878: 'autumn',\n",
       "  879: 'avail',\n",
       "  880: 'availability',\n",
       "  881: 'available',\n",
       "  882: 'avantgarde',\n",
       "  883: 'avenue',\n",
       "  884: 'average',\n",
       "  885: 'avert',\n",
       "  886: 'avocado',\n",
       "  887: 'avoid',\n",
       "  888: 'avoidance',\n",
       "  889: 'avoided',\n",
       "  890: 'avoiding',\n",
       "  891: 'avoids',\n",
       "  892: 'avowed',\n",
       "  893: 'awake',\n",
       "  894: 'awaken',\n",
       "  895: 'awakening',\n",
       "  896: 'awakens',\n",
       "  897: 'award',\n",
       "  898: 'awards',\n",
       "  899: 'aware',\n",
       "  900: 'awareness',\n",
       "  901: 'awash',\n",
       "  902: 'away',\n",
       "  903: 'awe',\n",
       "  904: 'awesome',\n",
       "  905: 'awful',\n",
       "  906: 'awfully',\n",
       "  907: 'awhile',\n",
       "  908: 'awkward',\n",
       "  909: 'awoke',\n",
       "  910: 'awry',\n",
       "  911: 'axioms',\n",
       "  912: 'ay',\n",
       "  913: 'aye',\n",
       "  914: 'ayurveda',\n",
       "  915: 'b',\n",
       "  916: 'babe',\n",
       "  917: 'babies',\n",
       "  918: 'baby',\n",
       "  919: 'babysitters',\n",
       "  920: 'bachelor',\n",
       "  921: 'bachelors',\n",
       "  922: 'back',\n",
       "  923: 'backbone',\n",
       "  924: 'backburner',\n",
       "  925: 'backed',\n",
       "  926: 'background',\n",
       "  927: 'backgrounds',\n",
       "  928: 'backing',\n",
       "  929: 'backpack',\n",
       "  930: 'backrooms',\n",
       "  931: 'backs',\n",
       "  932: 'backside',\n",
       "  933: 'backstage',\n",
       "  934: 'backward',\n",
       "  935: 'backwards',\n",
       "  936: 'bacon',\n",
       "  937: 'bad',\n",
       "  938: 'baddest',\n",
       "  939: 'badly',\n",
       "  940: 'badmouthing',\n",
       "  941: 'baffled',\n",
       "  942: 'baffling',\n",
       "  943: 'bag',\n",
       "  944: 'bagels',\n",
       "  945: 'baggy',\n",
       "  946: 'bags',\n",
       "  947: 'bailed',\n",
       "  948: 'bailiff',\n",
       "  949: 'bait',\n",
       "  950: 'baitandswitch',\n",
       "  951: 'bakers',\n",
       "  952: 'bakery',\n",
       "  953: 'balance',\n",
       "  954: 'balanced',\n",
       "  955: 'balances',\n",
       "  956: 'balancing',\n",
       "  957: 'balcony',\n",
       "  958: 'bald',\n",
       "  959: 'ball',\n",
       "  960: 'ballet',\n",
       "  961: 'ballistics',\n",
       "  962: 'balloon',\n",
       "  963: 'balloons',\n",
       "  964: 'ballot',\n",
       "  965: 'ballpark',\n",
       "  966: 'ballplayer',\n",
       "  967: 'balls',\n",
       "  968: 'ballsy',\n",
       "  969: 'ballymena',\n",
       "  970: 'balm',\n",
       "  971: 'baltic',\n",
       "  972: 'banal',\n",
       "  973: 'banana',\n",
       "  974: 'bananas',\n",
       "  975: 'band',\n",
       "  976: 'bandage',\n",
       "  977: 'bands',\n",
       "  978: 'bang',\n",
       "  979: 'bangalore',\n",
       "  980: 'bangs',\n",
       "  981: 'banish',\n",
       "  982: 'banished',\n",
       "  983: 'banishment',\n",
       "  984: 'bank',\n",
       "  985: 'bankrupt',\n",
       "  986: 'bankruptcy',\n",
       "  987: 'banks',\n",
       "  988: 'banners',\n",
       "  989: 'banoodles',\n",
       "  990: 'banquet',\n",
       "  991: 'banter',\n",
       "  992: 'bar',\n",
       "  993: 'barack',\n",
       "  994: 'baracks',\n",
       "  995: 'barbara',\n",
       "  996: 'barbarism',\n",
       "  997: 'barbarous',\n",
       "  998: 'barbie',\n",
       "  999: 'barbra',\n",
       "  ...})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_int, int_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13523"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/all.txt','r') as quotefile:\n",
    "    quotes = quotefile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/death.txt','r') as deathfile:\n",
    "    deathquotes = deathfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(quotes)\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13662,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, \n",
    "t.word_index, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = dict([(i,w) for (w,i) in t.word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('index_word.npy',index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 77, 47, 560, 238, 12, 100, 5, 16, 741, 2, 113, 2, 13, 2663]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(deathquotes)\n",
    "print(encoded_docs[0][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = np.load('data/embeddings_index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = embeddings_index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[9818]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('word_index.npy',t.word_index)\n",
    "# np.save('embedding_matrix.npy',embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13662, 100)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sequences, Next Sequences and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Death Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "step = 1\n",
    "seq_death = []\n",
    "next_seq_death = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "death_doc = encoded_docs[0]\n",
    "quote_len_death = len(death_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 23240\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, quote_len_death - maxlen, step):\n",
    "    seq_death.append(death_doc[i: i + maxlen])\n",
    "    next_seq_death.append(death_doc[i + maxlen])\n",
    "\n",
    "print('nb sequences:', len(seq_death))\n",
    "\n",
    "seq_death = np.asarray(seq_death)\n",
    "next_seq_death = np.asarray(next_seq_death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_death = np.zeros(next_seq_death.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_death"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funny Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/funny.txt','r') as funnyfile:\n",
    "    funnyquotes = funnyfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(funnyquotes)\n",
    "funny_doc = encoded_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "step = 1\n",
    "seq_funny = []\n",
    "next_seq_funny = []\n",
    "\n",
    "quote_len_funny = len(funny_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 24396\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, quote_len_funny - maxlen, step):\n",
    "    seq_funny.append(funny_doc[i: i + maxlen])\n",
    "    next_seq_funny.append(funny_doc[i + maxlen])\n",
    "\n",
    "print('nb sequences:', len(seq_funny))\n",
    "\n",
    "seq_funny = np.asarray(seq_funny)\n",
    "next_seq_funny = np.asarray(next_seq_funny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_funny = np.ones(next_seq_funny.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_funny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([seq_death,seq_funny])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47636, 100)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_death = np.reshape(labels_death,(labels_death.shape[0],1))\n",
    "labels_funny  = np.reshape(labels_funny,(labels_funny.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.vstack([labels_death,labels_funny])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47636, 1)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          1366200   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 100, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,429,133\n",
      "Trainable params: 62,933\n",
      "Non-trainable params: 1,366,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Train on 38108 samples, validate on 9528 samples\\nEpoch 1/10\\n38108/38108 [==============================] - 80s 2ms/step - \\nloss: 0.0391 - acc: 0.9859 - val_loss: 4.4262e-04 - val_acc: 0.9999'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=24)\n",
    "'''Train on 38108 samples, validate on 9528 samples\n",
    "Epoch 1/10\n",
    "38108/38108 [==============================] - 80s 2ms/step - \n",
    "loss: 0.0391 - acc: 0.9859 - val_loss: 4.4262e-04 - val_acc: 0.9999'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[    6,   343,    77, ...,    57,  1419,    40],\n",
       "        [  343,    77,     5, ...,  1419,    40,    42],\n",
       "        [   77,     5,    41, ...,    40,    42,  1038],\n",
       "        ..., \n",
       "        [   73,    13, 10360, ...,  3394,  1798,    23],\n",
       "        [   13, 10360,     8, ...,  1798,    23,     1],\n",
       "        [10360,     8,     1, ...,    23,     1,    53]]),\n",
       " array([  42, 1038,   75, ...,    1,   53, 1239]))"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_funny, next_seq_funny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = to_categorical(next_seq_funny, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = seq_funny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(LSTM(100))\n",
    "model.add(Dense(y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 100, 100)          1366200   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 100, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 50, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 13662)             1379862   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 13662)             0         \n",
      "=================================================================\n",
      "Total params: 2,808,894\n",
      "Trainable params: 1,442,694\n",
      "Non-trainable params: 1,366,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19516 samples, validate on 4880 samples\n",
      "Epoch 1/10\n",
      "19516/19516 [==============================] - 62s 3ms/step - loss: 8.8853 - acc: 0.0223 - val_loss: 7.5319 - val_acc: 0.0398\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "[537, 10202, 26, 15, 4964, 3674, 8, 201, 14, 3658, 3149, 4, 7, 57, 178, 151, 1946, 21, 34, 44, 126, 9, 18, 734, 1946, 3658, 3149, 5, 56, 5859, 598, 4, 245, 5, 79, 53, 7, 59, 32, 1306, 57, 59, 3, 335, 22, 53, 8, 201, 14, 42, 59, 42, 154, 44, 325, 7, 886, 195, 6, 131, 50, 1, 6661, 280, 65, 258, 53, 18, 10203, 44, 4823, 78, 4, 11, 5, 172, 3, 46, 10204, 262, 7, 58, 27, 6662, 6663, 763, 258, 81, 53, 658, 7, 124, 2951, 812, 5, 53, 21, 32, 103, 27]\n",
      "----- Generating with seed: ['book', 'carrie', 'was', 'my', 'alter', 'ego', 'in', 'real', 'life', 'sarah', 'jessica', 'and', 'i', 'dont', 'look', 'anything', 'alike', 'but', 'people', 'do', 'say', 'that', 'we', 'sound', 'alike', 'sarah', 'jessica', 'is', 'an', 'adorable', 'girl', 'and', 'she', 'is', 'very', 'funny', 'i', 'think', 'if', 'actors', 'dont', 'think', 'of', 'themselves', 'as', 'funny', 'in', 'real', 'life', 'they', 'think', 'they', 'cant', 'do', 'comedy', 'i', 'joke', 'around', 'a', 'lot', 'about', 'the', 'manic', 'times', 'because', 'theyre', 'funny', 'we', 'manics', 'do', 'outrageous', 'things', 'and', 'it', 'is', 'part', 'of', 'our', 'colorful', 'nature', 'i', 'like', 'all', 'jim', 'carrey', 'films', 'theyre', 'really', 'funny', 'yes', 'i', 'believe', 'blue', 'material', 'is', 'funny', 'but', 'if', 'thats', 'all']\n",
      "i i i i i i i i i i i i i i i i i i i i \n",
      "Epoch 2/10\n",
      "19516/19516 [==============================] - 60s 3ms/step - loss: 6.6552 - acc: 0.0422 - val_loss: 6.4387 - val_acc: 0.0398\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "[6, 1633, 29, 9, 7, 183, 3, 1294, 2, 775, 1, 2186, 6, 140, 405, 4, 7, 59, 7, 96, 438, 909, 15, 687, 448, 9899, 48, 2751, 4, 32, 218, 159, 2, 833, 228, 85, 11, 228, 134, 11, 53, 119, 680, 1, 53, 98, 5, 34, 560, 180, 45, 434, 12, 78, 353, 13, 8, 6, 2808, 4, 1, 5955, 62, 126, 968, 55, 24, 35, 1, 381, 1, 2962, 5, 74, 2, 310, 8, 6457, 65, 32, 61, 524, 187, 34, 134, 11, 53, 308, 2126, 9900, 1, 2640, 3, 11, 16, 94, 53, 24, 16, 379]\n",
      "----- Generating with seed: ['a', 'catholic', 'family', 'that', 'i', 'kind', 'of', 'liked', 'to', 'test', 'the', 'boundaries', 'a', 'little', 'bit', 'and', 'i', 'think', 'i', 'had', 'fun', 'watching', 'my', 'mom', 'laugh', 'comedys', 'so', 'subjective', 'and', 'if', 'someone', 'comes', 'to', 'watch', 'doesnt', 'get', 'it', 'doesnt', 'find', 'it', 'funny', 'then', 'fine', 'the', 'funny', 'thing', 'is', 'people', 'wont', 'let', 'me', 'pay', 'for', 'things', 'ill', 'be', 'in', 'a', 'restaurant', 'and', 'the', 'manager', 'will', 'say', 'oh', 'no', 'its', 'on', 'the', 'house', 'the', 'trick', 'is', 'always', 'to', 'write', 'in', 'pairs', 'because', 'if', 'at', 'least', 'two', 'people', 'find', 'it', 'funny', 'youve', 'immediately', 'halved', 'the', 'odds', 'of', 'it', 'not', 'being', 'funny', 'its', 'not', 'common']\n",
      "i i i i i i i i i i i i i i i i i i i i \n",
      "Epoch 3/10\n",
      "19516/19516 [==============================] - 60s 3ms/step - loss: 6.1472 - acc: 0.0422 - val_loss: 6.4396 - val_acc: 0.0398\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "[2, 6, 2234, 6567, 50, 69, 6568, 4, 707, 103, 182, 325, 761, 44, 129, 18, 255, 86, 140, 53, 589, 1, 279, 182, 10, 72, 60, 53, 4922, 64, 53, 6569, 5, 9, 4922, 19, 4923, 64, 6569, 32, 15, 1740, 52, 6, 4003, 227, 3, 601, 68, 71, 1825, 250, 222, 13, 6, 2531, 984, 12, 6, 3964, 65, 1412, 17, 2, 3853, 10, 42, 154, 81, 935, 10, 4, 10, 57, 17, 2, 1323, 9, 101, 2788, 13, 2845, 7, 57, 318, 674, 4, 68, 16, 502, 8, 34, 47, 44, 816, 24, 53, 373, 108, 3]\n",
      "----- Generating with seed: ['to', 'a', 'celebrity', 'tweeting', 'about', 'their', 'charities', 'and', 'shows', 'thats', 'why', 'comedy', 'writers', 'do', 'well', 'we', 'put', 'out', 'little', 'funny', 'ideas', 'the', 'reason', 'why', 'you', 'know', 'more', 'funny', 'dudes', 'than', 'funny', 'chicks', 'is', 'that', 'dudes', 'are', 'funnier', 'than', 'chicks', 'if', 'my', 'daughter', 'has', 'a', 'mediocre', 'sense', 'of', 'humor', 'im', 'just', 'gonna', 'tell', 'her', 'be', 'a', 'staff', 'writer', 'for', 'a', 'sitcom', 'because', 'theyll', 'have', 'to', 'hire', 'you', 'they', 'cant', 'really', 'fire', 'you', 'and', 'you', 'dont', 'have', 'to', 'produce', 'that', 'much', 'itll', 'be', 'awesome', 'i', 'dont', 'hate', 'humanity', 'and', 'im', 'not', 'interested', 'in', 'people', 'who', 'do', 'although', 'its', 'funny', 'actually', 'some', 'of']\n",
      "i i i i i i i i i i i i i i i i i i i i \n",
      "Epoch 4/10\n",
      "19516/19516 [==============================] - 60s 3ms/step - loss: 6.1119 - acc: 0.0422 - val_loss: 6.4577 - val_acc: 0.0398\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "[9830, 4, 416, 4, 551, 75, 23, 53, 78, 7, 163, 739, 51, 94, 53, 61, 317, 2509, 2, 13, 9831, 4, 6522, 195, 23, 6, 9832, 490, 7, 59, 24, 53, 2, 13, 2715, 23, 1438, 9, 19, 9833, 14, 38, 13, 1641, 4, 53, 27, 8, 1, 175, 136, 1, 699, 797, 5, 48, 53, 24, 1, 797, 7, 72, 155, 4, 24, 1, 797, 117, 10, 134, 1, 92, 6523, 48, 103, 39, 191, 1, 699, 2958, 48, 53, 68, 1051, 4, 7, 38, 13, 81, 53, 4, 557, 4, 7, 38, 107, 6524, 23, 681, 8]\n",
      "----- Generating with seed: ['hoops', 'and', 'friendship', 'and', 'coming', 'up', 'with', 'funny', 'things', 'i', 'got', 'attention', 'by', 'being', 'funny', 'at', 'school', 'pretending', 'to', 'be', 'retarded', 'and', 'jumping', 'around', 'with', 'a', 'deformed', 'hand', 'i', 'think', 'its', 'funny', 'to', 'be', 'delicate', 'with', 'subjects', 'that', 'are', 'explosive', 'life', 'can', 'be', 'dramatic', 'and', 'funny', 'all', 'in', 'the', 'same', 'day', 'the', 'middle', 'class', 'is', 'so', 'funny', 'its', 'the', 'class', 'i', 'know', 'best', 'and', 'its', 'the', 'class', 'where', 'you', 'find', 'the', 'most', 'pretension', 'so', 'thats', 'what', 'makes', 'the', 'middle', 'classes', 'so', 'funny', 'im', 'smart', 'and', 'i', 'can', 'be', 'really', 'funny', 'and', 'interesting', 'and', 'i', 'can', 'go', 'toetotoe', 'with', 'anybody', 'in']\n",
      "i i i i i i i i i i i i i i i i i i i i \n",
      "Epoch 5/10\n",
      "19516/19516 [==============================] - 61s 3ms/step - loss: 6.1002 - acc: 0.0422 - val_loss: 6.4762 - val_acc: 0.0398\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "[81, 205, 84, 2, 44, 11, 21, 275, 96, 2, 44, 11, 48, 7, 205, 17, 6, 439, 6, 969, 2666, 485, 4, 2271, 11, 4, 809, 623, 45, 108, 102, 24, 53, 65, 306, 74, 637, 9, 7, 274, 6, 457, 483, 7, 71, 205, 59, 9, 7, 95, 13, 8, 1, 487, 68, 58, 9892, 7, 120, 13, 446, 7, 57, 84, 2, 130, 6, 1115, 24, 53, 35, 423, 7, 57, 17, 2, 107, 2, 1, 2689, 7, 57, 17, 151, 327, 68, 1786, 680, 48, 9893, 68, 16, 1385, 68, 2063, 16, 115, 8, 15, 112]\n",
      "----- Generating with seed: ['really', 'didnt', 'want', 'to', 'do', 'it', 'but', 'everyone', 'had', 'to', 'do', 'it', 'so', 'i', 'didnt', 'have', 'a', 'choice', 'a', 'talent', 'agent', 'came', 'and', 'watched', 'it', 'and', 'later', 'gave', 'me', 'some', 'work', 'its', 'funny', 'because', 'id', 'always', 'known', 'that', 'i', 'wanted', 'a', 'movie', 'career', 'i', 'just', 'didnt', 'think', 'that', 'i', 'would', 'be', 'in', 'the', 'movies', 'im', 'like', 'bursting', 'i', 'should', 'be', 'working', 'i', 'dont', 'want', 'to', 'take', 'a', 'break', 'its', 'funny', 'on', 'set', 'i', 'dont', 'have', 'to', 'go', 'to', 'the', 'bathroom', 'i', 'dont', 'have', 'anything', 'wrong', 'im', 'perfectly', 'fine', 'so', 'throughandthrough', 'im', 'not', 'hungry', 'im', 'literally', 'not', 'even', 'in', 'my', 'own']\n",
      "i i i i i i i i i i i i i i i i i i i i \n",
      "Epoch 6/10\n",
      "19516/19516 [==============================] - 63s 3ms/step - loss: 6.0970 - acc: 0.0422 - val_loss: 6.4930 - val_acc: 0.0398\n",
      "\n",
      "----- Generating text after Epoch: 5\n",
      "----- diversity: 0.2\n",
      "[10, 44, 325, 10, 338, 51, 6, 246, 423, 3, 778, 55, 41, 81, 531, 10, 2, 13, 8, 9, 80, 2566, 94, 8, 80, 2566, 2609, 6, 706, 3, 2642, 9, 292, 933, 53, 2, 45, 39, 4228, 6, 53, 457, 54, 6, 80, 457, 5, 100, 312, 7, 154, 833, 99, 34, 166, 325, 22, 540, 22, 364, 1658, 94, 53, 7, 17, 2, 532, 321, 65, 11, 4707, 45, 7, 85, 325, 4351, 7, 71, 318, 681, 217, 94, 53, 103, 15, 311, 22, 6, 10096, 8, 6632, 8, 6633, 7, 205, 72, 9, 53, 167, 4508]\n",
      "----- Generating with seed: ['you', 'do', 'comedy', 'you', 'play', 'by', 'a', 'different', 'set', 'of', 'rules', 'no', 'one', 'really', 'wants', 'you', 'to', 'be', 'in', 'that', 'good', 'shape', 'being', 'in', 'good', 'shape', 'implies', 'a', 'level', 'of', 'vanity', 'that', 'isnt', 'necessarily', 'funny', 'to', 'me', 'what', 'separates', 'a', 'funny', 'movie', 'from', 'a', 'good', 'movie', 'is', 'something', 'personal', 'i', 'cant', 'watch', 'other', 'people', 'doing', 'comedy', 'as', 'soon', 'as', 'somebody', 'starts', 'being', 'funny', 'i', 'have', 'to', 'turn', 'off', 'because', 'it', 'upsets', 'me', 'i', 'get', 'comedy', 'indigestion', 'i', 'just', 'hate', 'anybody', 'else', 'being', 'funny', 'thats', 'my', 'job', 'as', 'a', 'fiveyearold', 'in', 'berlin', 'in', '1965', 'i', 'didnt', 'know', 'that', 'funny', 'women', 'existed']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i i i i i i i i i i i i i i i i i i i i \n",
      "Epoch 7/10\n",
      "12288/19516 [=================>............] - ETA: 21s - loss: 6.0783 - acc: 0.0422"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-520022814451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=1024,callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_word = np.load('index_word.npy')\n",
    "index_word = index_word.item()\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    # preds = preds[1:]\n",
    "    # preds = np.asarray(preds).astype('float64')\n",
    "    # preds = np.log(preds) / temperature\n",
    "    # exp_preds = np.exp(preds)\n",
    "    # preds = exp_preds / np.sum(exp_preds)\n",
    "    # probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(preds)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = np.random.randint(0, len(funny_doc) - maxlen - 1)\n",
    "    for diversity in [0.2]: # 0.5, 1.0, 1.2\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = funny_doc[start_index: start_index + maxlen]\n",
    "        print(sentence)\n",
    "        generated.join([str([index_word[value]]).join(' ') for value in sentence])\n",
    "        print('----- Generating with seed: %s'%[index_word[word] for word in sentence])\n",
    "        #sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(20):\n",
    "            x_pred = np.reshape(sentence,(1, maxlen))\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)\n",
    "            preds = preds[0]\n",
    "            # print(preds.shape)\n",
    "            next_index = sample(preds, diversity)\n",
    "            #print(next_index)\n",
    "            next_char = index_word[next_index]\n",
    "\n",
    "            generated.join(str(next_char))\n",
    "            sentence = np.append(sentence[1:],next_index)\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.write(\" \")\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
