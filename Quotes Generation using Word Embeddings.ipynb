{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "1. Load the Quotes dataset and choose 10 classes (topics) to generate new quotes from\n",
    "2. Clean the dataset - make lowercase, remove punctuations (apart from . and , )\n",
    "3. Save the cleaned quotes in 10 seperate files and an additional file with quotes from all topics\n",
    "4. Tokenize words using the file with all cleaned quotes\n",
    "5. Creating a Word Embedding Matrix for each Tokenized word using GloVe pretained word vectors   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Quotes Dataset\n",
    "\n",
    "RETRIEVED FROM HERE: . FEW CITATIONS, NOT USED FOR MACHINE LEARNING BEFORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age is an issue of mind over matter. If you do...</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anyone who stops learning is old, whether at t...</td>\n",
       "      <td>Henry Ford</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wrinkles should merely indicate where smiles h...</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True terror is to wake up one morning and disc...</td>\n",
       "      <td>Kurt Vonnegut</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A diplomat is a man who always remembers a wom...</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Quote         Author Topic\n",
       "0  Age is an issue of mind over matter. If you do...     Mark Twain   age\n",
       "1  Anyone who stops learning is old, whether at t...     Henry Ford   age\n",
       "2  Wrinkles should merely indicate where smiles h...     Mark Twain   age\n",
       "3  True terror is to wake up one morning and disc...  Kurt Vonnegut   age\n",
       "4  A diplomat is a man who always remembers a wom...   Robert Frost   age"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/quotes_all.csv',delimiter=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the following 10 topics to generate new quotes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = ['death' , 'family', 'freedom' , 'funny', 'happiness', 'life' , 'love', 'politics', 'science', 'success']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove all unneccessary punctuation and make everything lowercase. Since we want to retain '.' and ',' characters, we add spaces before them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I will never be an old man. To me, old age is always 15 years older than I am.\n",
      "Cleaned Sentence: i will never be an old man . to me , old age is always 15 years older than i am .\n"
     ]
    }
   ],
   "source": [
    "strip_special_chars = re.compile(\"[^A-Za-z0-9., ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    sentence = re.sub(strip_special_chars, \"\", string.lower())\n",
    "    return re.sub(r\"(\\w)([.,])\", r\"\\1 \\2\", sentence)\n",
    "\n",
    "sentence = 'I will never be an old man. To me, old age is always 15 years older than I am.'\n",
    "print('Original Sentence: %s'%sentence)\n",
    "print('Cleaned Sentence: %s'%cleanSentences(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Seperate files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transfer all cleaned quotes for the same topic to a seperate file. We also create a file to store cleaned quotes of all topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "for topic in topics:\n",
    "    quotes_data = data[data['Topic'].isin([topic])]\n",
    "\n",
    "    with open('data/%s.txt'%topic,'w+') as quotefile:\n",
    "        for quote in quotes_data['Quote']:\n",
    "            quotefile.writelines(cleanSentences(quote.lower()) + \" \")\n",
    "\n",
    "quotes_data = data[data['Topic'].isin(topics)]\n",
    "\n",
    "with open('data/all.txt','w+') as quotefile:\n",
    "    for quote in quotes_data['Quote']:\n",
    "        quotefile.writelines(cleanSentences(quote.lower()) + \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenize words\n",
    "\n",
    "Now we use the Keras tokenizer to give each unique word a token (integer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13668\n"
     ]
    }
   ],
   "source": [
    "with open('data/all.txt','r') as quotefile:\n",
    "    quotes = quotefile.readlines()\n",
    "\n",
    "t = Tokenizer(filters='')\n",
    "t.fit_on_texts(quotes)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a reverse dictionary to convert integers to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_word = dict([(i,w) for (w,i) in t.word_index.items()])\n",
    "np.save('data/index_word.npy',index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Word Embeddings using GloVe\n",
    "\n",
    "Download the GloVe pre-trained word vectors [here](http://nlp.stanford.edu/data/glove.6B.zip). We use the 100 dimensional vectors which is the file glove.6B.100d.txt. Save it in the data folder and then proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('data/glove.6B.100d.txt',encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "np.save('data/embeddings_index.npy',embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try and view the vector for '...' and its token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.10814   ,  0.35890999,  0.88518   , -0.44358999, -0.59486002,\n",
       "         0.33603001,  0.22067   , -0.22868   ,  0.31753999, -0.38008001], dtype=float32),\n",
       " 136)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.get('...')[:10], t.word_index['...']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our Word Embedding Matrix by choosing only the vectors for the words in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13668, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the vector for '...' at position 136:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10814   ,  0.35890999,  0.88518   , -0.44358999, -0.59486002,\n",
       "        0.33603001,  0.22067   , -0.22868   ,  0.31753999, -0.38008001])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[136][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data/embedding_matrix.npy',embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "1. Demonstrate generation of sequences and corresponding next sequences for training\n",
    "2. Explanation of the Model\n",
    "3. Callbacks for training, monitoring and saving weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sequences and Next Sequences\n",
    "\n",
    "Lets create sequences and next sequnces from the quotes from funny.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/funny.txt','r') as funnyfile:\n",
    "    funnyquotes = funnyfile.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the text to corresponding integers using the Tokenizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(funnyquotes)\n",
    "funny_doc = encoded_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create sequences of 100 words and their corresponding next word. We do this for all words in the file, one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 27436\n"
     ]
    }
   ],
   "source": [
    "maxlen = 100\n",
    "seq_funny = []\n",
    "next_seq_funny = []\n",
    "\n",
    "quote_len_funny = len(funny_doc)\n",
    "\n",
    "for i in range(0, quote_len_funny - maxlen, 1):\n",
    "    seq_funny.append(funny_doc[i: i + maxlen])\n",
    "    next_seq_funny.append(funny_doc[i + maxlen])\n",
    "\n",
    "print('sequences:', len(seq_funny))\n",
    "\n",
    "seq_funny = np.asarray(seq_funny)\n",
    "next_seq_funny = np.asarray(next_seq_funny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   8,  346,   79, ...,    5,   13,  536],\n",
       "        [ 346,   79,    7, ...,   13,  536,    1],\n",
       "        [  79,    7,   43, ...,  536,    1,    9],\n",
       "        ..., \n",
       "        [   2,   23,  645, ..., 1805,   25,    3],\n",
       "        [  23,  645,    5, ...,   25,    3,   55],\n",
       "        [ 645,    5,   78, ...,    3,   55, 1244]]),\n",
       " array([   1,    9,  141, ...,   55, 1244,    1]),\n",
       " (27436, 100),\n",
       " (27436,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_funny, next_seq_funny, seq_funny.shape, next_seq_funny.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = seq_funny\n",
    "y = to_categorical(next_seq_funny, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set trainable to  true to achieve results faster. Since we want to input a sequence of any length, we do not set a max length for the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=True)\n",
    "model.add(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(LSTM(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(y.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using softmax as the final layer and Adam to perform SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical crossentropy since the output is a one-hot encoded vector. Keeping track of accuracy, print the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         1366800   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 13668)             1380468   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 13668)             0         \n",
      "=================================================================\n",
      "Total params: 2,827,668\n",
      "Trainable params: 2,827,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Callback Functions and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train the weights on these sequences. But before that, we need to define a callback functions so that we can monitor the accuracy by completing a randomly chosen quote. We define another callback function to save the weights for an epoch:\n",
    "\n",
    "# CLEAN FUNCTION  BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = np.random.randint(0, len(funny_doc) - maxlen - 1)\n",
    "    sentence = funny_doc[start_index: start_index + maxlen]\n",
    "    \n",
    "    predicted = ''\n",
    "    original_sentence = ''.join([str(index_word[word])+' ' for word in sentence])\n",
    "    for i in range(maxlen):\n",
    "        x_pred = np.reshape(sentence,(1, -1))\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)\n",
    "        preds = preds[0]\n",
    "        next_index =  np.argmax(preds)\n",
    "        next_char = index_word[next_index]\n",
    "\n",
    "        sentence = np.append(sentence, next_index)\n",
    "        predicted = predicted + next_char + ' '\n",
    "\n",
    "        if i % (maxlen // 4) == 0:\n",
    "            sys.stdout.write(\"-\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    print('----- Input seed: %s'%original_sentence.split('.')[-1])\n",
    "    print('----- Output: %s'%predicted.split('.')[0])\n",
    "    sys.stdout.write(\"-----\\n\")\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"QG-funny-{epoch:02d}-{loss:.4f}-{acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we do a demo training for a single epoch with a batch size of 128 for demonstration. To achieve the same results as ours, train for atleast 30 epochs with a batch size of 24. The file [INSERT FILE NAME AND UPDATE LINK](https://github.com/krohak/QuoteGen/blob/master/embeddings.py) is used to train 10 sets of weights for different quote topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "27436/27436 [==============================] - 115s 4ms/step - loss: 5.8509 - acc: 0.0664\n",
      "\n",
      "Epoch 00001: loss improved from 5.88836 to 5.85088, saving model to QG-success-01-5.8509-0.0664.hdf5\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----\n",
      "----- Input seed:  everyones on \n",
      "----- Output: i \n",
      "-----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b93c81f98>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=1, batch_size= 1028, callbacks=[checkpoint, print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "1. Choose topics and load model and weights for topic\n",
    "3. Generate Quote!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = ['death', 'funny']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Model` class provides an API for ease in loading a model for any topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load_model function assumes that we have trained set of weights in the `data` directory called \n",
    "# CHANGE NAME qg-funny.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model_list = []\n",
    "\n",
    "# # ## Do for all docs except first\n",
    "# for topic in topics[1:]:\n",
    "#     model_funny = Model(vocab_size,topic)\n",
    "#     model = model_funny.load_model()\n",
    "#     model_list.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweak the callback function a little to accomodate various models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(sentence, model, maxlen = 10):\n",
    "    # for diversity in [1.0]: # 0.2, 0.5, 1.2\n",
    "    predicted = ''\n",
    "    original_sentence = ''.join([str(index_word[word])+' ' for word in sentence])\n",
    "    for i in range(maxlen):\n",
    "        x_pred = np.reshape(sentence,(1, -1))\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)\n",
    "        preds = preds[0]\n",
    "        next_index =  np.argmax(preds)\n",
    "        next_char = index_word[next_index]\n",
    "\n",
    "        sentence = np.append(sentence, next_index)\n",
    "        predicted = predicted + next_char + ' '\n",
    "\n",
    "        # sys.stdout.write(next_char)\n",
    "        if i % (maxlen // 4) == 0:\n",
    "            sys.stdout.write(\"-\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    print('----- Input seed: %s'%original_sentence.split('.')[-1])\n",
    "    print('----- Output: %s'%predicted.split('.')[0])\n",
    "    sys.stdout.write(\"-----\\n\")\n",
    "    return t.texts_to_sequences(str(original_sentence.split('.')[-1]) + ' ' + str(predicted.split('.')[0]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a seed length and max length for each model's output. We also choose a random sentence from the quotes of the first topic as the seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seedlen = 50\n",
    "maxlen = 50\n",
    "start_index = np.random.randint(0, len(funny_doc) - seedlen - 1)\n",
    "sentence = funny_doc[start_index: start_index + seedlen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the models sequentially:\n",
    "\n",
    "# MODEL WONT LOAD BECAUSE NO WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in model_list:\n",
    "    sentence = on_epoch_end(sentence,model,maxlen)\n",
    "    sentence = sentence[maxlen:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [generate.py](https://github.com/krohak/QuoteGen/blob/master/generate.py) to run the generation code for various combinations of topics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
