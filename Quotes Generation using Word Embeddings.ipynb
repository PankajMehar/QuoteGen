{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Topic Quotes Generation using Word Embeddings\n",
    "- Rohak Singhal, Ish Handa 2018/05/28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "1. Load the Quotes dataset and choose 10 topics to generate new quotes from\n",
    "2. Clean the dataset - make lowercase, remove punctuations (apart from . and , )\n",
    "3. Save the cleaned quotes in 10 seperate files and an additional file with quotes from all topics\n",
    "4. Tokenize words using the file with all cleaned quotes\n",
    "5. Creating a Word Embedding Matrix for each Tokenized word using GloVe pretained word vectors   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The Quotes Dataset\n",
    "\n",
    "Retrieved from [here](https://www.researchgate.net/publication/304742521_CSV_dataset_of_76000_quotes_suitable_for_quotes_recommender_systems_or_other_analysis). This is the first time that this dataset is being used for machine learning, since there are no citations involving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age is an issue of mind over matter. If you do...</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anyone who stops learning is old, whether at t...</td>\n",
       "      <td>Henry Ford</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wrinkles should merely indicate where smiles h...</td>\n",
       "      <td>Mark Twain</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True terror is to wake up one morning and disc...</td>\n",
       "      <td>Kurt Vonnegut</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A diplomat is a man who always remembers a wom...</td>\n",
       "      <td>Robert Frost</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Quote         Author Topic\n",
       "0  Age is an issue of mind over matter. If you do...     Mark Twain   age\n",
       "1  Anyone who stops learning is old, whether at t...     Henry Ford   age\n",
       "2  Wrinkles should merely indicate where smiles h...     Mark Twain   age\n",
       "3  True terror is to wake up one morning and disc...  Kurt Vonnegut   age\n",
       "4  A diplomat is a man who always remembers a wom...   Robert Frost   age"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/quotes_all.csv',delimiter=';')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of Quotes over the topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0353986c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "topic_counts = []\n",
    "for topic in data['Topic'].unique():\n",
    "    topic_counts.append(sum(data['Topic']==topic))\n",
    "\n",
    "plt.hist(topic_counts)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Topic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 50 files contain more than 900 quotes. Now to find out the distribution of words over quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEKCAYAAADq59mMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFaZJREFUeJzt3X/w3VV95/Hni4iAIhWWNJtNoMFtVjdQRRIpXbRFrRLF\nCtu6GqeutGWgM7Bbna5tibr+2NnM4rprLa4worUErSJWLSzKaKRgZztgTORHCEhJC1RiIGBXwa4T\nJL73j3u+cI35fnM/8L253++9z8fMnfv5nPv5fO45XwZenHM+93xSVUiS1MUBo66AJGn+MTwkSZ0Z\nHpKkzgwPSVJnhockqTPDQ5LUmeEhSerM8JAkdWZ4SJI6e9qoKzAsRx55ZC1btmzU1ZCkeWXz5s0P\nVdXCfR03tuGxbNkyNm3aNOpqSNK8kuTeQY5z2EqS1JnhIUnqzPCQJHVmeEiSOjM8JEmdGR6SpM4M\nD0lSZ4aHJKkzw0OS1NnY/sJ8nCw7/4vTfnbPBad1Ome64yWpC3sekqTODA9JUmeGhySpM+c85pCZ\n5jYkaS6x5yFJ6szwkCR1ZnhIkjozPCRJnTlhPmH88aCk2WDPQ5LUmeEhSerM8JAkdWZ4SJI6Mzwk\nSZ0ZHpKkzgwPSVJnhockqTPDQ5LUmeEhSeps6OGRZEGSm5Jc3faPSLIhyV3t/fC+Y9cm2ZbkziSn\n9pWvTLKlfXZhkgy73pKk6e2PnsdbgDv69s8Hrq2q5cC1bZ8kK4A1wLHAauCiJAvaORcDZwPL22v1\nfqi3JGkaQw2PJEuB04CP9RWfDqxv2+uBM/rKL6+qXVV1N7ANODHJYuCwqrqxqgq4rO8cSdIIDLvn\n8UHgD4Ef95Utqqodbft+YFHbXgJ8u++4+1rZkra9Z/lPSXJOkk1JNj344IOzUH1J0t4MLTySvAbY\nWVWbpzum9SRqtr6zqi6pqlVVtWrhwoWzdVlJ0h6G+TyPk4HXJnk1cDBwWJJPAg8kWVxVO9qQ1M52\n/HbgqL7zl7ay7W17z3JJ0ogMLTyqai2wFiDJKcDbqupNSd4PnAlc0N6vbKdcBXwqyQeAf0FvYnxj\nVe1O8nCSk4CvA28GPjSses830z3cSZKGaRRPErwAuCLJWcC9wOsBqmprkiuA24HHgPOqanc751zg\nUuAQ4Jr2kiSNyH4Jj6q6Hri+bX8XePk0x60D1u2lfBNw3PBqKEnqwmeYj4BDTZLmO5cnkSR1ZnhI\nkjozPCRJnRkekqTODA9JUmeGhySpM8NDktSZ4SFJ6szwkCR1ZnhIkjozPCRJnRkekqTODA9JUmeG\nhySpM8NDktSZ4SFJ6szwkCR1ZnhIkjozPCRJnRkekqTODA9JUmeGhySpM8NDktSZ4SFJ6szwkCR1\n9rRRV0Bzw7Lzv7jX8nsuOG0/10TSfGB4aKQMLWl+cthKktSZ4SFJ6sxhK83IYSVJe2PPQ5LUmT0P\nPSn2SKTJZs9DktSZ4SFJ6sxhqyGabmhHkuY7ex6SpM6GFh5JDk6yMcktSbYmeW8rPyLJhiR3tffD\n+85Zm2RbkjuTnNpXvjLJlvbZhUkyrHpLkvZtmD2PXcDLquoFwPHA6iQnAecD11bVcuDatk+SFcAa\n4FhgNXBRkgXtWhcDZwPL22v1EOstSdqHoc15VFUBP2i7B7ZXAacDp7Ty9cD1wB+18surahdwd5Jt\nwIlJ7gEOq6obAZJcBpwBXDOsumv0vBVYmtuGOmHeeg6bgZ8HPlxVX0+yqKp2tEPuBxa17SXAjX2n\n39fKftS29yzf2/edA5wDcPTRR89WMzQLvHlAGi9DnTCvqt1VdTywlF4v4rg9Pi96vZHZ+r5LqmpV\nVa1auHDhbF1WkrSH/XK3VVV9D7iO3lzFA0kWA7T3ne2w7cBRfactbWXb2/ae5ZKkERnm3VYLkzy7\nbR8CvAL4FnAVcGY77EzgyrZ9FbAmyUFJjqE3Mb6xDXE9nOSkdpfVm/vOkSSNwDDnPBYD69u8xwHA\nFVV1dZIbgCuSnAXcC7weoKq2JrkCuB14DDivqna3a50LXAocQm+i3MnyOcq5DWkyDBQeSX6hqrZ0\nuXBV3Qq8cC/l3wVePs0564B1eynfBBz302dIkkZh0GGri9oP/s5N8jNDrZEkac4bKDyq6iXAb9Kb\n0N6c5FNJXjHUmkmS5qyBJ8yr6i7gnfR+0PcrwIVJvpXk14dVOUnS3DRQeCR5fpI/Bu4AXgb8WlX9\n67b9x0OsnyRpDhr0bqsPAR8D3l5VP5wqrKrvJHnnUGom7YXLlkhzw6DhcRrww6lbZ5McABxcVf+v\nqj4xtNpJkuakQec8vkrvNxZTntHKJEkTaNDwOLiqplbIpW0/YzhVkiTNdYOGxz8lOWFqJ8lK4Icz\nHC9JGmODznm8Ffhsku8AAf458Iah1UqSNKcNFB5V9Y0kzwOe24rurKofDa9akqS5rMvCiC8ClrVz\nTkhCVV02lFpJHc20IKO38Uqzb9CFET8B/EvgZmBqpdsCDA9JmkCD9jxWASvak/8kSRNu0LutbqM3\nSS5J0sA9jyOB25NsBHZNFVbVa4dSK0nSnDZoeLxnmJWQJM0vg96q+7UkPwcsr6qvJnkGsGC4VZMk\nzVWDLsl+NvAXwEda0RLgL4dVKUnS3DbohPl5wMnAw/D4g6F+dliVkiTNbYOGx66qenRqJ8nT6P3O\nQ5I0gQYNj68leTtwSHt2+WeB/z28akmS5rJBw+N84EFgC/C7wJfoPc9ckjSBBr3b6sfAR9tLkjTh\nBl3b6m72MsdRVc+Z9RpJkua8LmtbTTkY+HfAEbNfHUnSfDDQnEdVfbfvtb2qPgi4zrUkTahBh61O\n6Ns9gF5PpMuzQCRJY2TQAPiffduPAfcAr5/12kiS5oVB77Z66bArIkmaPwYdtvr9mT6vqg/MTnWk\n2TfdI2p9PK305HW52+pFwFVt/9eAjcBdw6iUJGluGzQ8lgInVNUjAEneA3yxqt40rIpJkuauQZcn\nWQQ82rf/aCuTJE2gQXselwEbk3yh7Z8BrB9OlSRJc92gd1utS3IN8JJW9NtVddPwqiVJmssGHbYC\neAbwcFX9CXBfkmNmOjjJUUmuS3J7kq1J3tLKj0iyIcld7f3wvnPWJtmW5M4kp/aVr0yypX12YZJ0\nbKckaRYN+hjadwN/BKxtRQcCn9zHaY8B/6mqVgAnAeclWUFvefdrq2o5cG3bp322BjgWWA1clGTq\nOekXA2cDy9tr9UCtkyQNxaA9j38LvBb4J4Cq+g7wrJlOqKodVfXNtv0IcAe9Z5+fzhPzJevpzZ/Q\nyi+vql1VdTewDTgxyWLgsKq6saqK3vzLGUiSRmbQCfNHq6qSFECSZ3b5kiTLgBcCXwcWVdWO9tH9\nPHHX1hLgxr7T7mtlP2rbe5bPGdP9CE2SxtWg4XFFko8Az05yNvA7DPhgqCSHAp8D3lpVD/dPV/QH\n0mxIcg5wDsDRRx89W5fVmPKX59KTN+jdVv+jPbv8YeC5wLuqasO+zktyIL3g+POq+nwrfiDJ4qra\n0Yakdrby7cBRfacvbWXb2/ae5Xur5yXAJQCrVq2atVCSJP2kfc55JFmQ5Lqq2lBVf1BVbxswOAL8\nKXDHHmtfXQWc2bbPBK7sK1+T5KB2J9dyYGMb4no4yUntmm/uO0eSNAL77HlU1e4kP07yM1X1/Q7X\nPhn498CWJDe3srcDF9AbBjsLuJe2tHtVbU1yBXA7vTu1zquq3e28c4FLgUOAa9pLkjQig855/IBe\nCGyg3XEFUFW/N90JVfV/gOl+j/Hyac5ZB6zbS/km4LgB6ypJGrJBw+Pz7SVJ0szhkeToqvqHqnId\nK0nS4/Y1Yf6XUxtJPjfkukiS5ol9hUf/nMVzhlkRSdL8sa/wqGm2JUkTbF8T5i9I8jC9HsghbZu2\nX1V12FBrJ0mak2YMj6paMNPn0jhy2RJp37o8z0OSJGDw33lIE88eifQEex6SpM4MD0lSZw5bSU+R\nw1maRPY8JEmdGR6SpM4MD0lSZ4aHJKkzJ8ylIXEiXePMnockqTPDQ5LUmeEhSerM8JAkdWZ4SJI6\nMzwkSZ0ZHpKkzvydh7Sf+fsPjQN7HpKkzgwPSVJnhockqTPDQ5LUmeEhSerM8JAkdeatutIc4S28\nmk/seUiSOjM8JEmdGR6SpM4MD0lSZ4aHJKmzoYVHko8n2Znktr6yI5JsSHJXez+877O1SbYluTPJ\nqX3lK5NsaZ9dmCTDqrMkaTDD7HlcCqzeo+x84NqqWg5c2/ZJsgJYAxzbzrkoyYJ2zsXA2cDy9trz\nmpKk/Wxo4VFVfw384x7FpwPr2/Z64Iy+8suraldV3Q1sA05Mshg4rKpurKoCLus7R5I0Ivt7zmNR\nVe1o2/cDi9r2EuDbfcfd18qWtO09yyVJIzSyCfPWk6jZvGaSc5JsSrLpwQcfnM1LS5L67O/weKAN\nRdHed7by7cBRfcctbWXb2/ae5XtVVZdU1aqqWrVw4cJZrbgk6Qn7OzyuAs5s22cCV/aVr0lyUJJj\n6E2Mb2xDXA8nOandZfXmvnMkSSMytIURk3waOAU4Msl9wLuBC4ArkpwF3Au8HqCqtia5ArgdeAw4\nr6p2t0udS+/OrUOAa9pLkjRCQwuPqnrjNB+9fJrj1wHr9lK+CThuFqsmSXqKXJK9g+mWzJakSePy\nJJKkzgwPSVJnhockqTPDQ5LUmeEhSerM8JAkdWZ4SJI6MzwkSZ0ZHpKkzgwPSVJnhockqTPXtpLm\nuOnWVLvngtP2c02kJ9jzkCR1ZnhIkjpz2EqapxzO0ijZ85AkdWZ4SJI6MzwkSZ055yGNmZkel+x8\niGaL4SFNECfZNVsctpIkdWbPQ5I9EnVmz0OS1JnhIUnqzGErSdOa6c6tvXGYa3LY85AkdWZ4SJI6\nMzwkSZ0ZHpKkzgwPSVJnhockqTPDQ5LUmeEhSerMHwnuRdcfRknSpLHnIUnqzPCQJHU2b4atkqwG\n/gRYAHysqi4YcZUk7cGl3SfHvOh5JFkAfBh4FbACeGOSFaOtlSRNrnkRHsCJwLaq+vuqehS4HDh9\nxHWSpIk1X4atlgDf7tu/D/jFEdVF0ixxmGv+mi/hMZAk5wDntN0fJLlzhsOPBB4afq3mlElsM9ju\nkcv7hnt8nznT5v1sNtv9c4McNF/CYztwVN/+0lb2E6rqEuCSQS6YZFNVrZqd6s0Pk9hmsN2jrsf+\nNIlthtG0e77MeXwDWJ7kmCRPB9YAV424TpI0seZFz6OqHkvyH4Av07tV9+NVtXXE1ZKkiTUvwgOg\nqr4EfGkWLznQ8NaYmcQ2g+2eJJPYZhhBu1NV+/s7JUnz3HyZ85AkzSETFx5JVie5M8m2JOePuj6z\nKcnHk+xMcltf2RFJNiS5q70f3vfZ2vZ3uDPJqaOp9VOT5Kgk1yW5PcnWJG9p5ePe7oOTbExyS2v3\ne1v5WLcbeitOJLkpydVtfxLafE+SLUluTrKplY223VU1MS96k+1/BzwHeDpwC7Bi1PWaxfb9MnAC\ncFtf2X8Hzm/b5wPva9srWvsPAo5pf5cFo27Dk2jzYuCEtv0s4G9b28a93QEObdsHAl8HThr3dre2\n/D7wKeDqtj8Jbb4HOHKPspG2e9J6HmO9zElV/TXwj3sUnw6sb9vrgTP6yi+vql1VdTewjd7fZ16p\nqh1V9c22/QhwB70VCca93VVVP2i7B7ZXMebtTrIUOA34WF/xWLd5BiNt96SFx96WOVkyorrsL4uq\nakfbvh9Y1LbH7m+RZBnwQnr/Fz727W7DNzcDO4ENVTUJ7f4g8IfAj/vKxr3N0Psfg68m2dxW0oAR\nt3ve3Kqrp66qKslY3l6X5FDgc8Bbq+rhJI9/Nq7trqrdwPFJng18Iclxe3w+Vu1O8hpgZ1VtTnLK\n3o4Ztzb3eXFVbU/ys8CGJN/q/3AU7Z60nsdAy5yMmQeSLAZo7ztb+dj8LZIcSC84/ryqPt+Kx77d\nU6rqe8B1wGrGu90nA69Ncg+9IeeXJfkk491mAKpqe3vfCXyB3jDUSNs9aeExicucXAWc2bbPBK7s\nK1+T5KAkxwDLgY0jqN9Tkl4X40+BO6rqA30fjXu7F7YeB0kOAV4BfIsxbndVra2qpVW1jN6/u39V\nVW9ijNsMkOSZSZ41tQ28EriNUbd71HcR7O8X8Gp6d+T8HfCOUddnltv2aWAH8CN645xnAf8MuBa4\nC/gqcETf8e9of4c7gVeNuv5Pss0vpjcefCtwc3u9egLa/Xzgptbu24B3tfKxbndfW07hibutxrrN\n9O4OvaW9tk79d2vU7fYX5pKkziZt2EqSNAsMD0lSZ4aHJKkzw0OS1JnhIUnqzPDQ2EjyjrbC7K1t\n9dFfHHWdnooklyZ53RCvf3ySV/ftvyfJ24b1fRovLk+isZDkl4DX0Fthd1eSI+mtnKzpHQ+sYnaf\n0KkJYc9D42Ix8FBV7QKoqoeq6jsASVYm+VpbVO7LfUs6rGzPw7glyfvTnoOS5LeS/K+pCye5emot\npSSvTHJDkm8m+WxbU2vqeQvvbeVbkjyvlR+a5M9a2a1JfmOm6wwiyR8k+Ua73tRzPJYluSPJR1vv\n6yvtl+ckeVFfb+z9SW5rKyz8F+ANrfwN7fIrklyf5O+T/N6T/qehsWd4aFx8BTgqyd8muSjJr8Dj\n6159CHhdVa0EPg6sa+f8GfAfq+oFg3xB6828E/jVqjoB2ETv2RJTHmrlFwNTwz//Gfh+Vf1CVT0f\n+KsBrjNTHV5Jb7mJE+n1HFYm+eX28XLgw1V1LPA94Df62vm7VXU8sBugeo8keBfwmao6vqo+0459\nHnBqu/67299P+ikOW2ksVNUPkqwEXgK8FPhMek+K3AQcR28lUug9EGxHWxfq2dV7BgrAJ4BX7eNr\nTqL3oJ2/add6OnBD3+dTizJuBn69bf8qvXWYpur5f9vqsDNdZyavbK+b2v6h9ELjH4C7q+rmvjos\na+18VlVNXf9T9Ib3pvPF1nvblWQnvWW+7xuwbpoghofGRvWWKL8euD7JFnqLxW0GtlbVL/UfO7Wo\n4DQe4yd75QdPnUbvuRlvnOa8Xe19NzP/u7Wv68wkwH+rqo/8RGHvWSa7+op2A4c8ievveQ3/G6G9\ncthKYyHJc5Ms7ys6HriX3sJwC9uEOkkOTHJs9ZYx/16SF7fjf7Pv3HvoPSfjgCRH8cRT2G4ETk7y\n8+1az0zyr/ZRtQ3AeX31PPxJXmfKl4Hf6ZtrWZLeMx72qrXzkb47z9b0ffwIvUf3Sp0ZHhoXhwLr\nk9ye5FZ6w0LvaWP7rwPel+QWeqvu/pt2zm8DH07vaXzpu9bfAHcDtwMXAlOPuX0Q+C3g0+07bqA3\nRzCT/woc3iapbwFe2vE6H0lyX3vdUFVfoTf0dEPrXf0F+w6As4CPtnY+E/h+K7+O3gR5/4S5NBBX\n1ZV4fNjn6qo6bh+HzjtJDq32vPM2D7S4qt4y4mppnnM8Uxp/pyVZS+/f93vp9Xqkp8SehySpM+c8\nJEmdGR6SpM4MD0lSZ4aHJKkzw0OS1JnhIUnq7P8DbDMzZeETg1UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0386324b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_words = []\n",
    "for quote in data['Quote']:\n",
    "    counter = 0\n",
    "    for word in quote:\n",
    "        counter+=1\n",
    "    num_words.append(counter)\n",
    "\n",
    "plt.hist(num_words, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, Gauss. Centered approximately around 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the following 10 topics with over 900 quotes to generate new quotes from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = ['death' , 'family', 'freedom' , 'funny', 'happiness', 'life' , 'love', 'politics', 'science', 'success']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove all unneccessary punctuation and make everything lowercase. Since we want to retain '.' and ',' characters, we add spaces before them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I will never be an old man. To me, old age is always 15 years older than I am.\n",
      "Cleaned Sentence: i will never be an old man . to me , old age is always 15 years older than i am .\n"
     ]
    }
   ],
   "source": [
    "strip_special_chars = re.compile(\"[^A-Za-z0-9., ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    sentence = re.sub(strip_special_chars, \"\", string.lower())\n",
    "    return re.sub(r\"(\\w)([.,])\", r\"\\1 \\2\", sentence)\n",
    "\n",
    "sentence = 'I will never be an old man. To me, old age is always 15 years older than I am.'\n",
    "print('Original Sentence: %s'%sentence)\n",
    "print('Cleaned Sentence: %s'%cleanSentences(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Seperate files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transfer all cleaned quotes for the same topic to a seperate file. We also create a file to store cleaned quotes of all topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "for topic in topics:\n",
    "    quotes_data = data[data['Topic'].isin([topic])]\n",
    "\n",
    "    with open('data/%s.txt'%topic,'w+') as quotefile:\n",
    "        for quote in quotes_data['Quote']:\n",
    "            quotefile.writelines(cleanSentences(quote.lower()) + \" \")\n",
    "\n",
    "quotes_data = data[data['Topic'].isin(topics)]\n",
    "\n",
    "with open('data/all.txt','w+') as quotefile:\n",
    "    for quote in quotes_data['Quote']:\n",
    "        quotefile.writelines(cleanSentences(quote.lower()) + \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenize words\n",
    "\n",
    "Now we use the Keras tokenizer to give each unique word a token (integer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13668\n"
     ]
    }
   ],
   "source": [
    "with open('data/all.txt','r') as quotefile:\n",
    "    quotes = quotefile.readlines()\n",
    "\n",
    "t = Tokenizer(filters='')\n",
    "t.fit_on_texts(quotes)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a reverse dictionary to convert integers to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_word = dict([(i,w) for (w,i) in t.word_index.items()])\n",
    "np.save('data/index_word.npy',index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Word Embeddings using GloVe\n",
    "\n",
    "Download the GloVe pre-trained word vectors [here](http://nlp.stanford.edu/data/glove.6B.zip). We use the 100 dimensional vectors which is the file glove.6B.100d.txt. Save it in the data folder and then proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('data/glove.6B.100d.txt',encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "np.save('data/embeddings_index.npy',embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try and view the vector for '...' and its token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.10814   ,  0.35890999,  0.88518   , -0.44358999, -0.59486002,\n",
       "         0.33603001,  0.22067   , -0.22868   ,  0.31753999, -0.38008001], dtype=float32),\n",
       " 136)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.get('...')[:10], t.word_index['...']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our Word Embedding Matrix by choosing only the vectors for the words in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13668, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the vector for '...' at position 136:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10814   ,  0.35890999,  0.88518   , -0.44358999, -0.59486002,\n",
       "        0.33603001,  0.22067   , -0.22868   ,  0.31753999, -0.38008001])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[136][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data/embedding_matrix.npy',embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "1. Demonstrate generation of sequences and corresponding next sequences for training\n",
    "2. Explanation of the Model\n",
    "3. Callbacks for training, monitoring and saving weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sequences and Next Sequences\n",
    "\n",
    "Lets create sequences and next sequnces from the quotes from funny.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/funny.txt','r') as funnyfile:\n",
    "    funnyquotes = funnyfile.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the text to corresponding integers using the Tokenizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(funnyquotes)\n",
    "funny_doc = encoded_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create sequences of 100 words and their corresponding next word. We do this for all words in the file, one by one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 27436\n"
     ]
    }
   ],
   "source": [
    "maxlen = 100\n",
    "seq_funny = []\n",
    "next_seq_funny = []\n",
    "\n",
    "quote_len_funny = len(funny_doc)\n",
    "\n",
    "for i in range(0, quote_len_funny - maxlen, 1):\n",
    "    seq_funny.append(funny_doc[i: i + maxlen])\n",
    "    next_seq_funny.append(funny_doc[i + maxlen])\n",
    "\n",
    "print('sequences:', len(seq_funny))\n",
    "\n",
    "seq_funny = np.asarray(seq_funny)\n",
    "next_seq_funny = np.asarray(next_seq_funny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   8,  346,   79, ...,    5,   13,  536],\n",
       "        [ 346,   79,    7, ...,   13,  536,    1],\n",
       "        [  79,    7,   43, ...,  536,    1,    9],\n",
       "        ..., \n",
       "        [   2,   23,  645, ..., 1805,   25,    3],\n",
       "        [  23,  645,    5, ...,   25,    3,   55],\n",
       "        [ 645,    5,   78, ...,    3,   55, 1244]]),\n",
       " array([   1,    9,  141, ...,   55, 1244,    1]),\n",
       " (27436, 100),\n",
       " (27436,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_funny, next_seq_funny, seq_funny.shape, next_seq_funny.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Model\n",
    "We convert the next words to one hot encoded vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = seq_funny\n",
    "y = to_categorical(next_seq_funny, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Sequential model is a [linear stack of layers](https://keras.io/getting-started/sequential-model-guide/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the embedding layer, we set trainable to  true to achieve results faster. Since we want to input a sequence of any length, we do not set a max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=True)\n",
    "model.add(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM hidden layers with 100 memory cells and a Dense layer the size of the vocab was added to the model. The output of the Dense layer is a vector of probabilities for each word in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(LSTM(100))\n",
    "model.add(Dense(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using softmax as the final layer and Adam to perform SGD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical crossentropy since the output is a one-hot encoded vector. Keeping track of accuracy, print the model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1366800   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 13668)             1380468   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 13668)             0         \n",
      "=================================================================\n",
      "Total params: 2,827,668\n",
      "Trainable params: 2,827,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Callback Functions and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train the weights on these sequences. But before that, we need to define a callback function so that we can monitor the accuracy by completing a randomly chosen quote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = np.random.randint(0, len(funny_doc) - maxlen - 1)\n",
    "    sentence = funny_doc[start_index: start_index + maxlen]\n",
    "    \n",
    "    predicted = ''\n",
    "    original_sentence = ''.join([str(index_word[word])+' ' for word in sentence])\n",
    "    for i in range(maxlen):\n",
    "        x_pred = np.reshape(sentence,(1, -1))\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)\n",
    "        preds = preds[0]\n",
    "        next_index =  np.argmax(preds)\n",
    "        next_char = index_word[next_index]\n",
    "\n",
    "        sentence = np.append(sentence, next_index)\n",
    "        predicted = predicted + next_char + ' '\n",
    "\n",
    "        if i % (maxlen // 4) == 0:\n",
    "            sys.stdout.write(\"-\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    print('----- Input seed: %s'%original_sentence.split('.')[-1])\n",
    "    print('----- Output: %s'%predicted.split('.')[0])\n",
    "    sys.stdout.write(\"-----\\n\")\n",
    "        \n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define another callback function to save the weights for an epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"trained_weights/QG-funny-{epoch:02d}-{loss:.4f}-{acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we do a demo training for a single epoch with a batch size of 1028 for demonstration. To achieve the same results as ours, train for atleast 30 epochs with a batch size of 24. The file [train.py](https://github.com/krohak/QuoteGen/blob/master/train.py) is used to train 10 sets of weights for different quote topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "27436/27436 [==============================] - 117s 4ms/step - loss: 8.3232 - acc: 0.0403\n",
      "\n",
      "Epoch 00001: loss improved from inf to 8.32320, saving model to data/QG-funny-01-8.3232-0.0403.hdf5\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----\n",
      "----- Input seed:  i have only been funny about seventy four per cent of the time \n",
      "----- Output: to to to to to to to to to to to\n",
      "-----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f033f266630>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=1, batch_size= 1028, callbacks=[checkpoint, print_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "1. Choose topics and load model and weights for topic\n",
    "3. Generate Quote!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = ['funny', 'death']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Model` class provides an API for ease in loading a model for any topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Model import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load_model function assumes that we have trained set of weights in the `data` directory called QG-**topic**.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_list = []\n",
    "\n",
    "# ## Do for all docs except first\n",
    "for topic in topics[1:]:\n",
    "    model_funny = Model(vocab_size,topic)\n",
    "    model = model_funny.load_model()\n",
    "    model_list.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweak the callback function a little to accomodate various models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(sentence, model, maxlen = 10):\n",
    "    # for diversity in [1.0]: # 0.2, 0.5, 1.2\n",
    "    predicted = ''\n",
    "    original_sentence = ''.join([str(index_word[word])+' ' for word in sentence])\n",
    "    for i in range(maxlen):\n",
    "        x_pred = np.reshape(sentence,(1, -1))\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)\n",
    "        preds = preds[0]\n",
    "        next_index =  np.argmax(preds)\n",
    "        next_char = index_word[next_index]\n",
    "\n",
    "        sentence = np.append(sentence, next_index)\n",
    "        predicted = predicted + next_char + ' '\n",
    "\n",
    "        # sys.stdout.write(next_char)\n",
    "        if i % (maxlen // 4) == 0:\n",
    "            sys.stdout.write(\"-\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    print('----- Input seed: %s'%original_sentence.split('.')[-1])\n",
    "    print('----- Output: %s'%predicted.split('.')[0])\n",
    "    sys.stdout.write(\"-----\\n\")\n",
    "    return t.texts_to_sequences(str(original_sentence.split('.')[-1]) + ' ' + str(predicted.split('.')[0]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a seed length and max length for each model's output. We also choose a random sentence from the quotes of the first topic as the seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seedlen = 50\n",
    "maxlen = 50\n",
    "start_index = np.random.randint(0, len(funny_doc) - seedlen - 1)\n",
    "sentence = funny_doc[start_index: start_index + seedlen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the models sequentially:\n",
    "\n",
    "<img src='https://c2.staticflickr.com/2/1752/28521512978_0375be5429_z.jpg' width='425'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funny', 'death']\n",
      "-----\n",
      "----- Input seed:  i think people like comedies and i \n",
      "----- Output: have to sweat and roll up to spend money on whom death is outlawed \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "print([str(topic) for topic in topics])\n",
    "for model in model_list:\n",
    "    sentence = on_epoch_end(sentence,model,maxlen)\n",
    "    sentence = sentence[maxlen:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [generate.py](https://github.com/krohak/QuoteGen/blob/master/generate.py) to run the generation code for various combinations of topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Select 2 topics from 'death' ,'family', 'funny', 'freedom' , 'life' , 'love', 'happiness', 'science', 'success', 'politics'\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.check_output('python generate.py',shell=True).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-----\\n----- Input seed:  i cant even remember our latest lie \\n----- Output: in the love of a perfumed garden , but it can be a moving sea between the shores of your souls \\n-----\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.check_output('python generate.py death love',shell=True).decode(\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
